{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a3c5da3",
   "metadata": {},
   "source": [
    "# Lab Assignment Four: Multi-Layer Perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b34d0d",
   "metadata": {},
   "source": [
    "In this lab, we will compare the performance of multi-layer perceptrons programmed  via your own various implementations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ab909",
   "metadata": {},
   "source": [
    "## Team Members:\n",
    "1) Mohammed Ahmed Abdelrazek Aboelela.\n",
    "\n",
    "2) Yihan Zhou\n",
    "\n",
    "3) Sofiya Chaku\n",
    "\n",
    "4) Naim Barnett"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a87aeab",
   "metadata": {},
   "source": [
    "## Dataset Description (as per Kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d42f21e",
   "metadata": {},
   "source": [
    "The data here are taken from the DP03 and DP05 tables of the 2015 American Community Survey 5-year estimates. The full datasets and much more can be found at the American Factfinder website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5815b2",
   "metadata": {},
   "source": [
    "The classification task we will be performing is to predict, for each county, what the child poverty rate will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1035d8",
   "metadata": {},
   "source": [
    "## Load, Split, and Balance (1.5 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fbc348",
   "metadata": {},
   "source": [
    "[.5 points] Load the data into memory and save it to a pandas data frame. Do not normalize or one-hot encode any of the features until asked to do so.  Remove any observations that having missing data. Encode any string data as integers for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baaa85df",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Importing all the required libraries throughout the lab'''\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import warnings\n",
    "import re\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a909d343",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3220 entries, 0 to 3219\n",
      "Data columns (total 37 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   CensusId         3220 non-null   int64  \n",
      " 1   State            3220 non-null   object \n",
      " 2   County           3220 non-null   object \n",
      " 3   TotalPop         3220 non-null   int64  \n",
      " 4   Men              3220 non-null   int64  \n",
      " 5   Women            3220 non-null   int64  \n",
      " 6   Hispanic         3220 non-null   float64\n",
      " 7   White            3220 non-null   float64\n",
      " 8   Black            3220 non-null   float64\n",
      " 9   Native           3220 non-null   float64\n",
      " 10  Asian            3220 non-null   float64\n",
      " 11  Pacific          3220 non-null   float64\n",
      " 12  Citizen          3220 non-null   int64  \n",
      " 13  Income           3219 non-null   float64\n",
      " 14  IncomeErr        3219 non-null   float64\n",
      " 15  IncomePerCap     3220 non-null   int64  \n",
      " 16  IncomePerCapErr  3220 non-null   int64  \n",
      " 17  Poverty          3220 non-null   float64\n",
      " 18  ChildPoverty     3219 non-null   float64\n",
      " 19  Professional     3220 non-null   float64\n",
      " 20  Service          3220 non-null   float64\n",
      " 21  Office           3220 non-null   float64\n",
      " 22  Construction     3220 non-null   float64\n",
      " 23  Production       3220 non-null   float64\n",
      " 24  Drive            3220 non-null   float64\n",
      " 25  Carpool          3220 non-null   float64\n",
      " 26  Transit          3220 non-null   float64\n",
      " 27  Walk             3220 non-null   float64\n",
      " 28  OtherTransp      3220 non-null   float64\n",
      " 29  WorkAtHome       3220 non-null   float64\n",
      " 30  MeanCommute      3220 non-null   float64\n",
      " 31  Employed         3220 non-null   int64  \n",
      " 32  PrivateWork      3220 non-null   float64\n",
      " 33  PublicWork       3220 non-null   float64\n",
      " 34  SelfEmployed     3220 non-null   float64\n",
      " 35  FamilyWork       3220 non-null   float64\n",
      " 36  Unemployment     3220 non-null   float64\n",
      "dtypes: float64(27), int64(8), object(2)\n",
      "memory usage: 930.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55221</td>\n",
       "      <td>26745</td>\n",
       "      <td>28476</td>\n",
       "      <td>2.6</td>\n",
       "      <td>75.8</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>26.5</td>\n",
       "      <td>23986</td>\n",
       "      <td>73.6</td>\n",
       "      <td>20.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>195121</td>\n",
       "      <td>95314</td>\n",
       "      <td>99807</td>\n",
       "      <td>4.5</td>\n",
       "      <td>83.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>26.4</td>\n",
       "      <td>85953</td>\n",
       "      <td>81.5</td>\n",
       "      <td>12.3</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>26932</td>\n",
       "      <td>14497</td>\n",
       "      <td>12435</td>\n",
       "      <td>4.6</td>\n",
       "      <td>46.2</td>\n",
       "      <td>46.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>24.1</td>\n",
       "      <td>8597</td>\n",
       "      <td>71.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>22604</td>\n",
       "      <td>12073</td>\n",
       "      <td>10531</td>\n",
       "      <td>2.2</td>\n",
       "      <td>74.5</td>\n",
       "      <td>21.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>28.8</td>\n",
       "      <td>8294</td>\n",
       "      <td>76.8</td>\n",
       "      <td>16.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Blount</td>\n",
       "      <td>57710</td>\n",
       "      <td>28512</td>\n",
       "      <td>29198</td>\n",
       "      <td>8.6</td>\n",
       "      <td>87.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>34.9</td>\n",
       "      <td>22189</td>\n",
       "      <td>82.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CensusId    State   County  TotalPop    Men  Women  Hispanic  White  Black  \\\n",
       "0      1001  Alabama  Autauga     55221  26745  28476       2.6   75.8   18.5   \n",
       "1      1003  Alabama  Baldwin    195121  95314  99807       4.5   83.1    9.5   \n",
       "2      1005  Alabama  Barbour     26932  14497  12435       4.6   46.2   46.7   \n",
       "3      1007  Alabama     Bibb     22604  12073  10531       2.2   74.5   21.4   \n",
       "4      1009  Alabama   Blount     57710  28512  29198       8.6   87.9    1.5   \n",
       "\n",
       "   Native  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  Employed  \\\n",
       "0     0.4  ...   0.5          1.3         1.8         26.5     23986   \n",
       "1     0.6  ...   1.0          1.4         3.9         26.4     85953   \n",
       "2     0.2  ...   1.8          1.5         1.6         24.1      8597   \n",
       "3     0.4  ...   0.6          1.5         0.7         28.8      8294   \n",
       "4     0.3  ...   0.9          0.4         2.3         34.9     22189   \n",
       "\n",
       "   PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \n",
       "0         73.6        20.9           5.5         0.0           7.6  \n",
       "1         81.5        12.3           5.8         0.4           7.5  \n",
       "2         71.8        20.8           7.3         0.1          17.6  \n",
       "3         76.8        16.1           6.7         0.4           8.3  \n",
       "4         82.0        13.5           4.2         0.4           7.7  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Loading the data into memory and saving it to a pandas dataframe\"\"\"\n",
    "data = pd.read_csv(\"Data/acs2015_county_data.csv\", low_memory=False)\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b284e3b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3218 entries, 0 to 3219\n",
      "Data columns (total 37 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   CensusId         3218 non-null   int64  \n",
      " 1   State            3218 non-null   object \n",
      " 2   County           3218 non-null   object \n",
      " 3   TotalPop         3218 non-null   int64  \n",
      " 4   Men              3218 non-null   int64  \n",
      " 5   Women            3218 non-null   int64  \n",
      " 6   Hispanic         3218 non-null   float64\n",
      " 7   White            3218 non-null   float64\n",
      " 8   Black            3218 non-null   float64\n",
      " 9   Native           3218 non-null   float64\n",
      " 10  Asian            3218 non-null   float64\n",
      " 11  Pacific          3218 non-null   float64\n",
      " 12  Citizen          3218 non-null   int64  \n",
      " 13  Income           3218 non-null   float64\n",
      " 14  IncomeErr        3218 non-null   float64\n",
      " 15  IncomePerCap     3218 non-null   int64  \n",
      " 16  IncomePerCapErr  3218 non-null   int64  \n",
      " 17  Poverty          3218 non-null   float64\n",
      " 18  ChildPoverty     3218 non-null   float64\n",
      " 19  Professional     3218 non-null   float64\n",
      " 20  Service          3218 non-null   float64\n",
      " 21  Office           3218 non-null   float64\n",
      " 22  Construction     3218 non-null   float64\n",
      " 23  Production       3218 non-null   float64\n",
      " 24  Drive            3218 non-null   float64\n",
      " 25  Carpool          3218 non-null   float64\n",
      " 26  Transit          3218 non-null   float64\n",
      " 27  Walk             3218 non-null   float64\n",
      " 28  OtherTransp      3218 non-null   float64\n",
      " 29  WorkAtHome       3218 non-null   float64\n",
      " 30  MeanCommute      3218 non-null   float64\n",
      " 31  Employed         3218 non-null   int64  \n",
      " 32  PrivateWork      3218 non-null   float64\n",
      " 33  PublicWork       3218 non-null   float64\n",
      " 34  SelfEmployed     3218 non-null   float64\n",
      " 35  FamilyWork       3218 non-null   float64\n",
      " 36  Unemployment     3218 non-null   float64\n",
      "dtypes: float64(27), int64(8), object(2)\n",
      "memory usage: 955.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55221</td>\n",
       "      <td>26745</td>\n",
       "      <td>28476</td>\n",
       "      <td>2.6</td>\n",
       "      <td>75.8</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>26.5</td>\n",
       "      <td>23986</td>\n",
       "      <td>73.6</td>\n",
       "      <td>20.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>195121</td>\n",
       "      <td>95314</td>\n",
       "      <td>99807</td>\n",
       "      <td>4.5</td>\n",
       "      <td>83.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>26.4</td>\n",
       "      <td>85953</td>\n",
       "      <td>81.5</td>\n",
       "      <td>12.3</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>26932</td>\n",
       "      <td>14497</td>\n",
       "      <td>12435</td>\n",
       "      <td>4.6</td>\n",
       "      <td>46.2</td>\n",
       "      <td>46.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>24.1</td>\n",
       "      <td>8597</td>\n",
       "      <td>71.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>22604</td>\n",
       "      <td>12073</td>\n",
       "      <td>10531</td>\n",
       "      <td>2.2</td>\n",
       "      <td>74.5</td>\n",
       "      <td>21.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>28.8</td>\n",
       "      <td>8294</td>\n",
       "      <td>76.8</td>\n",
       "      <td>16.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Blount</td>\n",
       "      <td>57710</td>\n",
       "      <td>28512</td>\n",
       "      <td>29198</td>\n",
       "      <td>8.6</td>\n",
       "      <td>87.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>34.9</td>\n",
       "      <td>22189</td>\n",
       "      <td>82.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3215</th>\n",
       "      <td>72145</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Vega Baja</td>\n",
       "      <td>56858</td>\n",
       "      <td>27379</td>\n",
       "      <td>29479</td>\n",
       "      <td>96.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>13660</td>\n",
       "      <td>78.3</td>\n",
       "      <td>17.6</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>72147</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Vieques</td>\n",
       "      <td>9130</td>\n",
       "      <td>4585</td>\n",
       "      <td>4545</td>\n",
       "      <td>96.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2860</td>\n",
       "      <td>44.5</td>\n",
       "      <td>41.6</td>\n",
       "      <td>13.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3217</th>\n",
       "      <td>72149</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Villalba</td>\n",
       "      <td>24685</td>\n",
       "      <td>12086</td>\n",
       "      <td>12599</td>\n",
       "      <td>99.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>26.9</td>\n",
       "      <td>6795</td>\n",
       "      <td>59.2</td>\n",
       "      <td>27.5</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>72151</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Yabucoa</td>\n",
       "      <td>36279</td>\n",
       "      <td>17648</td>\n",
       "      <td>18631</td>\n",
       "      <td>99.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>29.5</td>\n",
       "      <td>8083</td>\n",
       "      <td>65.1</td>\n",
       "      <td>27.6</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3219</th>\n",
       "      <td>72153</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Yauco</td>\n",
       "      <td>39474</td>\n",
       "      <td>19047</td>\n",
       "      <td>20427</td>\n",
       "      <td>99.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>24.6</td>\n",
       "      <td>8923</td>\n",
       "      <td>68.0</td>\n",
       "      <td>27.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3218 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CensusId        State     County  TotalPop    Men  Women  Hispanic  \\\n",
       "0         1001      Alabama    Autauga     55221  26745  28476       2.6   \n",
       "1         1003      Alabama    Baldwin    195121  95314  99807       4.5   \n",
       "2         1005      Alabama    Barbour     26932  14497  12435       4.6   \n",
       "3         1007      Alabama       Bibb     22604  12073  10531       2.2   \n",
       "4         1009      Alabama     Blount     57710  28512  29198       8.6   \n",
       "...        ...          ...        ...       ...    ...    ...       ...   \n",
       "3215     72145  Puerto Rico  Vega Baja     56858  27379  29479      96.4   \n",
       "3216     72147  Puerto Rico    Vieques      9130   4585   4545      96.7   \n",
       "3217     72149  Puerto Rico   Villalba     24685  12086  12599      99.7   \n",
       "3218     72151  Puerto Rico    Yabucoa     36279  17648  18631      99.8   \n",
       "3219     72153  Puerto Rico      Yauco     39474  19047  20427      99.5   \n",
       "\n",
       "      White  Black  Native  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  \\\n",
       "0      75.8   18.5     0.4  ...   0.5          1.3         1.8         26.5   \n",
       "1      83.1    9.5     0.6  ...   1.0          1.4         3.9         26.4   \n",
       "2      46.2   46.7     0.2  ...   1.8          1.5         1.6         24.1   \n",
       "3      74.5   21.4     0.4  ...   0.6          1.5         0.7         28.8   \n",
       "4      87.9    1.5     0.3  ...   0.9          0.4         2.3         34.9   \n",
       "...     ...    ...     ...  ...   ...          ...         ...          ...   \n",
       "3215    3.4    0.1     0.0  ...   1.2          1.3         0.3         32.0   \n",
       "3216    2.9    0.0     0.0  ...  10.8          0.0         1.4         14.0   \n",
       "3217    0.0    0.0     0.0  ...   3.2          0.0         3.3         26.9   \n",
       "3218    0.2    0.0     0.0  ...   2.3          2.3         1.5         29.5   \n",
       "3219    0.5    0.0     0.0  ...   1.6          0.7         3.1         24.6   \n",
       "\n",
       "      Employed  PrivateWork  PublicWork  SelfEmployed  FamilyWork  \\\n",
       "0        23986         73.6        20.9           5.5         0.0   \n",
       "1        85953         81.5        12.3           5.8         0.4   \n",
       "2         8597         71.8        20.8           7.3         0.1   \n",
       "3         8294         76.8        16.1           6.7         0.4   \n",
       "4        22189         82.0        13.5           4.2         0.4   \n",
       "...        ...          ...         ...           ...         ...   \n",
       "3215     13660         78.3        17.6           4.1         0.0   \n",
       "3216      2860         44.5        41.6          13.6         0.3   \n",
       "3217      6795         59.2        27.5          13.1         0.2   \n",
       "3218      8083         65.1        27.6           7.3         0.0   \n",
       "3219      8923         68.0        27.6           4.4         0.0   \n",
       "\n",
       "      Unemployment  \n",
       "0              7.6  \n",
       "1              7.5  \n",
       "2             17.6  \n",
       "3              8.3  \n",
       "4              7.7  \n",
       "...            ...  \n",
       "3215          15.2  \n",
       "3216          12.2  \n",
       "3217          25.9  \n",
       "3218          24.3  \n",
       "3219          27.1  \n",
       "\n",
       "[3218 rows x 37 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Removing observations that have missing data\"\"\"\n",
    "#Replace all blank strings will null to be dropped\n",
    "data.replace('', np.nan, inplace=True)\n",
    "#Remove all rows with null values\n",
    "data.dropna(inplace=True)\n",
    "#Find duplicate instances\n",
    "duplicates = data[data.duplicated()]\n",
    "#Remove all duplicates\n",
    "data = data.drop_duplicates()\n",
    "#After removal \n",
    "data.info()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e9f05a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observations for each state before incoding was:\n",
      "Texas                   253\n",
      "Georgia                 159\n",
      "Virginia                133\n",
      "Kentucky                120\n",
      "Missouri                115\n",
      "Kansas                  105\n",
      "Illinois                102\n",
      "North Carolina          100\n",
      "Iowa                     99\n",
      "Tennessee                95\n",
      "Nebraska                 93\n",
      "Indiana                  92\n",
      "Ohio                     88\n",
      "Minnesota                87\n",
      "Michigan                 83\n",
      "Mississippi              82\n",
      "Puerto Rico              78\n",
      "Oklahoma                 77\n",
      "Arkansas                 75\n",
      "Wisconsin                72\n",
      "Pennsylvania             67\n",
      "Alabama                  67\n",
      "Florida                  67\n",
      "South Dakota             66\n",
      "Louisiana                64\n",
      "Colorado                 64\n",
      "New York                 62\n",
      "California               58\n",
      "Montana                  56\n",
      "West Virginia            55\n",
      "North Dakota             53\n",
      "South Carolina           46\n",
      "Idaho                    44\n",
      "Washington               39\n",
      "Oregon                   36\n",
      "New Mexico               33\n",
      "Alaska                   29\n",
      "Utah                     29\n",
      "Maryland                 24\n",
      "Wyoming                  23\n",
      "New Jersey               21\n",
      "Nevada                   17\n",
      "Maine                    16\n",
      "Arizona                  15\n",
      "Massachusetts            14\n",
      "Vermont                  14\n",
      "New Hampshire            10\n",
      "Connecticut               8\n",
      "Rhode Island              5\n",
      "Hawaii                    4\n",
      "Delaware                  3\n",
      "District of Columbia      1\n",
      "Name: State, dtype: int64\n",
      "The number of observations for each state after incoding becomes:\n",
      "43    253\n",
      "10    159\n",
      "46    133\n",
      "17    120\n",
      "25    115\n",
      "16    105\n",
      "13    102\n",
      "33    100\n",
      "15     99\n",
      "42     95\n",
      "27     93\n",
      "14     92\n",
      "35     88\n",
      "23     87\n",
      "22     83\n",
      "24     82\n",
      "51     78\n",
      "36     77\n",
      "3      75\n",
      "49     72\n",
      "38     67\n",
      "0      67\n",
      "9      67\n",
      "41     66\n",
      "18     64\n",
      "5      64\n",
      "32     62\n",
      "4      58\n",
      "26     56\n",
      "48     55\n",
      "34     53\n",
      "40     46\n",
      "12     44\n",
      "47     39\n",
      "37     36\n",
      "31     33\n",
      "1      29\n",
      "44     29\n",
      "20     24\n",
      "50     23\n",
      "30     21\n",
      "28     17\n",
      "19     16\n",
      "2      15\n",
      "21     14\n",
      "45     14\n",
      "29     10\n",
      "6       8\n",
      "39      5\n",
      "11      4\n",
      "7       3\n",
      "8       1\n",
      "Name: State, dtype: int64\n",
      "The number of observations for each County before incoding was:\n",
      "47           31\n",
      "Jefferson    26\n",
      "Franklin     25\n",
      "Jackson      24\n",
      "Lincoln      24\n",
      "             ..\n",
      "Gladwin       1\n",
      "Eaton         1\n",
      "Clare         1\n",
      "Cheboygan     1\n",
      "Yauco         1\n",
      "Name: County, Length: 1926, dtype: int64\n",
      "The number of observations for each County after incoding becomes:\n",
      "64      31\n",
      "36      26\n",
      "29      25\n",
      "35      24\n",
      "140     24\n",
      "        ..\n",
      "847      1\n",
      "845      1\n",
      "844      1\n",
      "842      1\n",
      "1925     1\n",
      "Name: County, Length: 1926, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3218 entries, 0 to 3219\n",
      "Data columns (total 37 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   CensusId         3218 non-null   int64  \n",
      " 1   State            3218 non-null   int64  \n",
      " 2   County           3218 non-null   int64  \n",
      " 3   TotalPop         3218 non-null   int64  \n",
      " 4   Men              3218 non-null   int64  \n",
      " 5   Women            3218 non-null   int64  \n",
      " 6   Hispanic         3218 non-null   float64\n",
      " 7   White            3218 non-null   float64\n",
      " 8   Black            3218 non-null   float64\n",
      " 9   Native           3218 non-null   float64\n",
      " 10  Asian            3218 non-null   float64\n",
      " 11  Pacific          3218 non-null   float64\n",
      " 12  Citizen          3218 non-null   int64  \n",
      " 13  Income           3218 non-null   float64\n",
      " 14  IncomeErr        3218 non-null   float64\n",
      " 15  IncomePerCap     3218 non-null   int64  \n",
      " 16  IncomePerCapErr  3218 non-null   int64  \n",
      " 17  Poverty          3218 non-null   float64\n",
      " 18  ChildPoverty     3218 non-null   float64\n",
      " 19  Professional     3218 non-null   float64\n",
      " 20  Service          3218 non-null   float64\n",
      " 21  Office           3218 non-null   float64\n",
      " 22  Construction     3218 non-null   float64\n",
      " 23  Production       3218 non-null   float64\n",
      " 24  Drive            3218 non-null   float64\n",
      " 25  Carpool          3218 non-null   float64\n",
      " 26  Transit          3218 non-null   float64\n",
      " 27  Walk             3218 non-null   float64\n",
      " 28  OtherTransp      3218 non-null   float64\n",
      " 29  WorkAtHome       3218 non-null   float64\n",
      " 30  MeanCommute      3218 non-null   float64\n",
      " 31  Employed         3218 non-null   int64  \n",
      " 32  PrivateWork      3218 non-null   float64\n",
      " 33  PublicWork       3218 non-null   float64\n",
      " 34  SelfEmployed     3218 non-null   float64\n",
      " 35  FamilyWork       3218 non-null   float64\n",
      " 36  Unemployment     3218 non-null   float64\n",
      "dtypes: float64(27), int64(10)\n",
      "memory usage: 955.3 KB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Incoding string data as integers\"\"\"\n",
    "#Incoding states' names\n",
    "print(\"The number of observations for each state before incoding was:\")\n",
    "print(data['State'].value_counts())\n",
    "states_names = data[\"State\"].unique()\n",
    "states_id = np.arange(states_names.size)\n",
    "data.replace(states_names,states_id, inplace=True)\n",
    "data.replace(data[\"State\"].unique(),states_id, inplace=True)\n",
    "print(\"The number of observations for each state after incoding becomes:\")\n",
    "print(data['State'].value_counts())\n",
    "\n",
    "#Incoding county names\n",
    "print(\"The number of observations for each County before incoding was:\")\n",
    "print(data['County'].value_counts())\n",
    "county_names = data[\"County\"].unique()\n",
    "county_id = np.arange(county_names.size)\n",
    "data.replace(county_names,county_id, inplace=True)\n",
    "print(\"The number of observations for each County after incoding becomes:\")\n",
    "print(data['County'].value_counts())\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eddacb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43    253\n",
      "10    159\n",
      "46    133\n",
      "17    120\n",
      "25    115\n",
      "16    105\n",
      "13    102\n",
      "33    100\n",
      "15     99\n",
      "42     95\n",
      "27     93\n",
      "14     92\n",
      "35     88\n",
      "23     87\n",
      "22     83\n",
      "24     82\n",
      "51     78\n",
      "36     77\n",
      "3      75\n",
      "49     72\n",
      "38     67\n",
      "0      67\n",
      "9      67\n",
      "41     66\n",
      "18     64\n",
      "5      64\n",
      "32     62\n",
      "4      58\n",
      "26     56\n",
      "48     55\n",
      "34     53\n",
      "40     46\n",
      "12     44\n",
      "47     39\n",
      "37     36\n",
      "31     33\n",
      "1      29\n",
      "44     29\n",
      "20     24\n",
      "50     23\n",
      "30     21\n",
      "28     17\n",
      "19     16\n",
      "2      15\n",
      "21     14\n",
      "45     14\n",
      "29     10\n",
      "6       8\n",
      "39      5\n",
      "11      4\n",
      "7       3\n",
      "8       1\n",
      "Name: State, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data.replace(data[\"State\"].unique(),states_id, inplace=True)\n",
    "print(data[\"State\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3356d506",
   "metadata": {},
   "source": [
    "[.5 points] Assume you are equally interested in the classification performance for each class in the dataset. Split the dataset into 80% for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95cae079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data matrix: (3218, 36)\n",
      "The shape of the target variable: (3218,)\n",
      "The training matrix and target shapes: (2574, 36) & (2574,)\n",
      "The testing matrix and target shapes: (644, 36) & (644,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Split the dataset into 80% for training and 20% for testing.\"\"\"\n",
    "# Creating our data matrix (X) and our target variable (y) that we will work on from the dataframe we have\n",
    "X = data.copy().drop([\"ChildPoverty\"],axis=1).to_numpy()\n",
    "#y = data[['Credit_Score']].to_numpy()\n",
    "y = data.ChildPoverty.to_numpy()\n",
    "print(\"The shape of the data matrix: \" + str(X.shape))\n",
    "print(\"The shape of the target variable: \" + str(y.shape))\n",
    "# Dividing the data into training and testing data using an 80% training and 20% testing split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.copy(), y.copy(), test_size=0.2, random_state=0)\n",
    "print(\"The training matrix and target shapes: \" + str(X_train.shape)+ ' & ' + str(y_train.shape))\n",
    "print(\"The testing matrix and target shapes: \" + str(X_test.shape)+ ' & ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ba2fd7",
   "metadata": {},
   "source": [
    "[.5 points] Balance the dataset so that about the same number of instances are within each class. Choose a method for balancing the dataset and explain your reasoning for selecting this method. One option is to choose quantization thresholds for the \"ChildPoverty\" variable that equally divide the data into four classes. Should balancing of the dataset be done for both the training and testing set? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921cf163",
   "metadata": {},
   "source": [
    "We choose to quantize threshold for the \"ChildPoverty\" variable that equally divide the data into four classes. We do this balancing using the entire dataset altogether, and then also apply it to training and testing to have the same evaluation criteria and division, because otherwise, the thresholds for the training and the testing sets will be different and it will be meaningless to predict the target on the test set using a model that was trained for the thresholds of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eba43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Balancing the dataset by finding quantization thresholds for the Child Poverty variable\"\"\"\n",
    "#a function that acts on the dataset, and finds the three threshold points that define the four quarters of the variable\n",
    "def get_threshold_points():\n",
    "    i = 0;\n",
    "    threshold_points = []\n",
    "    while True:\n",
    "        if int(data[data[\"ChildPoverty\"] < i][\"ChildPoverty\"].size) >= math.floor(int(data[\"ChildPoverty\"].size)/4): \n",
    "            threshold_points.append(i)\n",
    "            while True:\n",
    "                if int(data[data[\"ChildPoverty\"] < i][\"ChildPoverty\"].size) >= math.floor(int(data[\"ChildPoverty\"].size)/2): \n",
    "                    threshold_points.append(i)\n",
    "                    while True:\n",
    "                        if int(data[data[\"ChildPoverty\"] < i][\"ChildPoverty\"].size) >= math.floor(int(data[\"ChildPoverty\"].size)*3/4): \n",
    "                            threshold_points.append(i)\n",
    "                            break\n",
    "                        else: i += 0.001\n",
    "                    break\n",
    "                else: i += 0.001\n",
    "            break\n",
    "        else: i += 0.001\n",
    "\n",
    "    return threshold_points\n",
    "\n",
    "threshold_pts = get_threshold_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc2c88f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.30099999999694, 22.70000000000476, 30.00000000001368]\n",
      "The number of instances in the first quarter: 812\n",
      "The number of instances in the second quarter: 804\n",
      "The number of instances in the third quarter: 799\n",
      "The number of instances in the fourth quarter: 803\n",
      "The \"almost perfect\" number of instances tha should be in each quarter: 804\n",
      "The number of observations that set on the edge of the first quarter: 19\n"
     ]
    }
   ],
   "source": [
    "print(threshold_pts)\n",
    "first_quarter = data[data[\"ChildPoverty\"] < threshold_pts[0]]\n",
    "other_than_first = data[data[\"ChildPoverty\"] > threshold_pts[0]]\n",
    "second_quarter = other_than_first[other_than_first[\"ChildPoverty\"] < threshold_pts[1]]\n",
    "other_than_second = data[data[\"ChildPoverty\"] > threshold_pts[1]]\n",
    "third_quarter = other_than_second[other_than_second[\"ChildPoverty\"] < threshold_pts[2]]\n",
    "fourth_quarter = data[data[\"ChildPoverty\"] > threshold_pts[2]]\n",
    "print(\"The number of instances in the first quarter:\",int(first_quarter[\"ChildPoverty\"].size))\n",
    "print(\"The number of instances in the second quarter:\",int(second_quarter[\"ChildPoverty\"].size))\n",
    "print(\"The number of instances in the third quarter:\",int(third_quarter[\"ChildPoverty\"].size))\n",
    "print(\"The number of instances in the fourth quarter:\",int(fourth_quarter[\"ChildPoverty\"].size))\n",
    "print('''The \"almost perfect\" number of instances tha should be in each quarter:''',math.floor(int(data[\"ChildPoverty\"].size)/4))\n",
    "#as we can see, we can't perfectly divide them into four equal sets because on the edge, at point 16.8, we have 19 \n",
    "#observations which will all belong to the first quarter, making it exceed. Since we want to have more flexibility\n",
    "#and we want to be more inclined towards classifying people in the less poverty rate bracket, we will assign them\n",
    "#to the first quarter, but in general, it will depend on the problem in hand.5\n",
    "print(\"The number of observations that set on the edge of the first quarter:\",int(data[data[\"ChildPoverty\"] == 16.8][\"ChildPoverty\"].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b874e7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 1. ... 0. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "#Applying the quantization to our target value \"Child Poverty\" in the training set\n",
    "y_train[y_train < threshold_pts[0]] = 0\n",
    "y_train[(y_train > threshold_pts[0]) & (y_train < threshold_pts[1])] = 1\n",
    "y_train[(y_train > threshold_pts[1]) & (y_train < threshold_pts[2])] = 2\n",
    "y_train[y_train > threshold_pts[2]] = 3\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eff15191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 0. 3. 2. 2. 1. 2. 2. 1. 0. 0. 3. 1. 1. 0. 0. 1. 1. 3. 3. 2. 1. 1. 1.\n",
      " 0. 2. 3. 0. 3. 2. 3. 3. 0. 2. 2. 1. 1. 2. 2. 1. 3. 0. 2. 3. 2. 2. 0. 3.\n",
      " 3. 2. 0. 0. 3. 2. 2. 1. 3. 2. 1. 1. 1. 2. 2. 3. 1. 2. 3. 3. 0. 1. 1. 3.\n",
      " 1. 1. 3. 3. 1. 0. 1. 2. 0. 0. 1. 3. 3. 3. 2. 1. 3. 2. 1. 0. 3. 3. 0. 0.\n",
      " 1. 3. 3. 0. 2. 1. 2. 0. 1. 0. 0. 2. 2. 0. 3. 1. 0. 3. 3. 0. 3. 2. 0. 3.\n",
      " 0. 3. 0. 0. 1. 1. 1. 1. 0. 1. 2. 1. 3. 2. 1. 1. 0. 3. 3. 2. 1. 2. 0. 0.\n",
      " 1. 3. 3. 0. 2. 0. 2. 0. 3. 0. 1. 1. 1. 1. 3. 2. 3. 0. 1. 1. 0. 3. 2. 1.\n",
      " 0. 3. 3. 3. 0. 1. 2. 1. 2. 2. 3. 3. 3. 0. 2. 3. 2. 0. 1. 2. 1. 3. 3. 3.\n",
      " 1. 0. 1. 0. 1. 0. 3. 1. 1. 3. 2. 2. 0. 2. 3. 1. 2. 1. 0. 1. 3. 2. 3. 1.\n",
      " 1. 3. 3. 2. 3. 1. 1. 0. 2. 1. 3. 0. 1. 3. 1. 3. 2. 2. 0. 3. 3. 2. 2. 3.\n",
      " 2. 3. 1. 2. 3. 0. 3. 1. 2. 0. 1. 3. 1. 1. 1. 0. 1. 1. 0. 3. 0. 3. 3. 0.\n",
      " 1. 1. 3. 3. 0. 2. 2. 0. 0. 0. 3. 3. 1. 2. 2. 2. 3. 0. 3. 2. 2. 1. 3. 2.\n",
      " 0. 1. 2. 3. 0. 0. 2. 3. 3. 3. 1. 1. 1. 1. 3. 1. 3. 3. 0. 2. 1. 0. 3. 2.\n",
      " 1. 1. 3. 3. 0. 3. 1. 3. 2. 1. 3. 0. 1. 3. 2. 2. 1. 1. 0. 0. 0. 3. 0. 1.\n",
      " 2. 3. 1. 0. 3. 0. 0. 3. 1. 2. 1. 0. 0. 0. 0. 1. 0. 0. 2. 2. 0. 1. 0. 2.\n",
      " 3. 2. 3. 2. 3. 3. 3. 1. 3. 0. 0. 2. 2. 1. 0. 3. 3. 2. 3. 2. 1. 1. 3. 0.\n",
      " 0. 0. 0. 2. 1. 2. 1. 2. 0. 1. 3. 0. 2. 2. 2. 0. 2. 0. 0. 0. 1. 0. 2. 3.\n",
      " 1. 2. 2. 0. 3. 3. 3. 2. 3. 2. 2. 1. 3. 2. 2. 1. 3. 3. 1. 0. 0. 3. 0. 2.\n",
      " 2. 0. 2. 1. 0. 2. 0. 2. 2. 0. 1. 3. 3. 3. 2. 2. 0. 2. 2. 0. 1. 3. 1. 2.\n",
      " 0. 0. 0. 1. 1. 1. 3. 1. 3. 3. 2. 0. 0. 3. 0. 1. 3. 2. 3. 1. 3. 3. 0. 1.\n",
      " 2. 1. 1. 3. 0. 0. 2. 1. 2. 1. 1. 2. 1. 3. 1. 0. 1. 2. 0. 2. 2. 3. 1. 2.\n",
      " 2. 3. 2. 2. 2. 0. 2. 3. 1. 1. 1. 2. 1. 1. 1. 2. 0. 3. 1. 2. 2. 3. 2. 0.\n",
      " 0. 0. 0. 1. 3. 2. 3. 0. 2. 2. 3. 0. 0. 2. 2. 1. 2. 0. 0. 1. 3. 3. 3. 1.\n",
      " 1. 3. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 3. 2. 3. 2. 0. 1. 1. 1. 1. 0. 3. 2.\n",
      " 0. 3. 0. 1. 3. 2. 1. 3. 3. 3. 0. 0. 3. 2. 2. 2. 3. 3. 0. 0. 3. 1. 1. 3.\n",
      " 2. 0. 1. 1. 3. 3. 0. 0. 3. 0. 1. 1. 1. 2. 1. 1. 2. 3. 1. 2. 2. 1. 1. 2.\n",
      " 3. 3. 2. 0. 3. 2. 3. 0. 1. 0. 2. 1. 1. 1. 3. 3. 2. 1. 2. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Applying the quantization to our target value \"Child Poverty\" in the testing set\n",
    "y_test[y_test < threshold_pts[0]] = 0\n",
    "y_test[(y_test > threshold_pts[0]) & (y_test < threshold_pts[1])] = 1\n",
    "y_test[(y_test > threshold_pts[1]) & (y_test < threshold_pts[2])] = 2\n",
    "y_test[y_test > threshold_pts[2]] = 3\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83132541",
   "metadata": {},
   "source": [
    "## Pre-processing (2.5 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8b2a0c",
   "metadata": {},
   "source": [
    "There are a number of version of the two layer perceptron covered in class. When using the example two layer network from class be sure that you use: (1) vectorized computation, (2) mini-batching, (3) cross entropy loss, and (4) proper Glorot initialization, at a minimum.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc437d",
   "metadata": {},
   "source": [
    "[.5 points] Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Do not normalize or one-hot encode the data (not yet). Be sure that training converges by graphing the loss function versus the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f91df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Using the code from the lecture\"\"\"\n",
    "\n",
    "\"\"\"Defining the base class of the neural network\"\"\"\n",
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_) # reshape to be W\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden)\n",
    "        b2 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, b1, b2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2, b1, b2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "        \n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C\n",
    "\n",
    "        return gradW1, gradW2, gradb1, gradb2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2, self.b1, self.b2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6973caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving the class with feedforward, fit and predict\"\"\"\n",
    "# now let's add in the following functions:\n",
    "#    feedforward\n",
    "#    fit and predict\n",
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1, W2, b1, b2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # need to vectorize this computation!\n",
    "        # See additional code and derivation below!\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2, self.b1, self.b2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X_data,self.W1,self.W2, self.b1, self.b2)\n",
    "            \n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            gradW1, gradW2, gradb1, gradb2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, Y_enc=Y_enc,\n",
    "                                              W1=self.W1, W2=self.W2)\n",
    "\n",
    "            self.W1 -= self.eta * gradW1\n",
    "            self.W2 -= self.eta * gradW2\n",
    "            self.b1 -= self.eta * gradb1\n",
    "            self.b2 -= self.eta * gradb2\n",
    "            \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03f9484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving the class with Vectorized inputs and outputs and calculations\"\"\"\n",
    "class TwoLayerPerceptronVectorized(TwoLayerPerceptron):\n",
    "    # just need a different gradient calculation\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "        \n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C * 2\n",
    "        gradW2 += W2 * self.l2_C * 2 \n",
    "\n",
    "        return gradW1, gradW2, gradb1, grabdb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b91e8357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving the class with mini-batching added\"\"\"\n",
    "class TLPMiniBatch(TwoLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.1, \n",
    "                 decrease_iter = 10, shuffle=True, \n",
    "                 minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.decrease_iter = decrease_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "\n",
    "        # start momentum at zero for previous updates\n",
    "        rho_W1_prev = np.zeros(self.W1.shape) # for momentum\n",
    "        rho_W2_prev = np.zeros(self.W2.shape) # for momentum\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            self.val_cost_ = []\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # decrease at certain epochs\n",
    "            eta = self.eta * self.decrease_const**(np.floor(i/self.decrease_iter))\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                       self.b1,\n",
    "                                                       self.b2\n",
    "                                                      )\n",
    "                \n",
    "                cost = self._cost(A3,Y_enc[:, idx],self.W1,self.W2)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradb1, gradb2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, \n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1,W2=self.W2)\n",
    "\n",
    "                # momentum calculations\n",
    "                rho_W1, rho_W2 = eta * gradW1, eta * gradW2\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev)) # update with momentum\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev)) # update with momentum\n",
    "                self.b1 -= eta * gradb1\n",
    "                self.b2 -= eta * gradb2\n",
    "                rho_W1_prev, rho_W2_prev = rho_W1, rho_W2\n",
    "                \n",
    "\n",
    "            self.cost_.append(np.mean(mini_cost))\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                yhat = self.predict(X_test)\n",
    "                self.val_score_.append(accuracy_score(y_test,yhat))\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da352907",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving the class with cross entropy added\"\"\"\n",
    "class TLPMiniBatchCrossEntropy(TLPMiniBatch):\n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = (A3-Y_enc) # <- this is only line that changed\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "        \n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C\n",
    "\n",
    "        return gradW1, gradW2, gradb1, gradb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "337da750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving a class with Glorot Initialization\"\"\"\n",
    "class TLPBetterInitial(TLPMiniBatchCrossEntropy):             \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_))\n",
    "\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden)) \n",
    "        \n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        b2 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88ebbdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 200/200"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.36024844720496896\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Training on our data\"\"\"\n",
    "vals = {'n_hidden':50, \n",
    "         'C':1e-2, 'epochs':200, 'eta':0.01, \n",
    "         'alpha':0.1, 'decrease_const':0.1,\n",
    "         'decrease_iter':20,\n",
    "         'minibatches':len(X_train)/256,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn = TLPBetterInitial(**vals)\n",
    "\n",
    "nn.fit(X_train, y_train, print_progress=1)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d82e3826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Plotting the cost function versus the number of epochs\"\"\" ##inspired by the lecture too\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5c0e7",
   "metadata": {},
   "source": [
    "[.5 points] Now normalize the continuous numeric feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c719f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 200/200"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.36024844720496896\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Normalizing the continuous numeric feature data\"\"\"\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scaler.transform(X_train)\n",
    "scaler.fit(X_test)\n",
    "scaler.transform(X_test)\n",
    "\n",
    "nn = TLPBetterInitial(**vals)\n",
    "\n",
    "nn.fit(X_train, y_train, print_progress=50)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test,yhat))\n",
    "\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f84b71b",
   "metadata": {},
   "source": [
    "[.5 points] Now normalize the continuous numeric feature data AND one hot encode the categorical data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2fae9d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m y_train \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39m_encode_labels(y_train)\n\u001b[1;32m      5\u001b[0m y_test \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39m_encode_labels(y_test)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m yhat \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m,accuracy_score(y_test,yhat))\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mTLPMiniBatch.fit\u001b[0;34m(self, X, y, print_progress, XY_test)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\" Learn weights from training data. With mini-batch\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m X_data, y_data \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcopy(), y\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 19\u001b[0m Y_enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# init weights and setup matrices\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_ \u001b[38;5;241m=\u001b[39m X_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mTwoLayerPerceptronBase._encode_labels\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_labels\u001b[39m(y):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124;03m\"\"\"Encode labels into one-hot representation\"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     onehot \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m onehot\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/reshape.py:989\u001b[0m, in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    987\u001b[0m     result \u001b[38;5;241m=\u001b[39m concat(with_dummies, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 989\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_get_dummies_1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix_sep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdummy_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/reshape.py:1013\u001b[0m, in \u001b[0;36m_get_dummies_1d\u001b[0;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m concat\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;66;03m# Series avoids inconsistent NaN handling\u001b[39;00m\n\u001b[0;32m-> 1013\u001b[0m codes, levels \u001b[38;5;241m=\u001b[39m factorize_from_iterable(\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1016\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:451\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    449\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m     manager \u001b[38;5;241m=\u001b[39m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/construction.py:598\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure, allow_2d)\u001b[0m\n\u001b[1;32m    595\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n\u001b[1;32m    596\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m maybe_infer_to_datetimelike(subarr)\n\u001b[0;32m--> 598\u001b[0m subarr \u001b[38;5;241m=\u001b[39m \u001b[43m_sanitize_ndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subarr, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001b[39;00m\n\u001b[1;32m    602\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mdtype, dtype)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/construction.py:649\u001b[0m, in \u001b[0;36m_sanitize_ndim\u001b[0;34m(result, data, dtype, index, allow_2d)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allow_2d:\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData must be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;66;03m# i.e. PandasDtype(\"O\")\u001b[39;00m\n\u001b[1;32m    653\u001b[0m     result \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: Data must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "\"\"\"One Hot Encoding the data we have\"\"\"\n",
    "nn = TLPBetterInitial(**vals)\n",
    "\n",
    "y_train = nn._encode_labels(y_train)\n",
    "y_test = nn._encode_labels(y_test)\n",
    "nn.fit(X_train, y_train, print_progress=50)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test,yhat))\n",
    "\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#Hi "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6ffb5",
   "metadata": {},
   "source": [
    "[1 points] Compare the performance of the three models you just trained. Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have) different performances.  \n",
    "\n",
    "Use one-hot encoding and normalization on the dataset for the remainder of this lab assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe4f0c",
   "metadata": {},
   "source": [
    "## Modeling (5 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca02a70",
   "metadata": {},
   "source": [
    "[1 points] Add support for a third layer in the multi-layer perceptron. Add support for saving (and plotting after training is completed) the average magnitude of the gradient for each layer, for each epoch. For magnitude calculation, you are free to use either the average absolute values or the L1/L2 norm. Quantify the performance of the model and graph the magnitudes for each layer versus the number of epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a8315",
   "metadata": {},
   "source": [
    "[1 points] Repeat the previous step, adding support for a fourth layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3eb84f",
   "metadata": {},
   "source": [
    "[1 points] Repeat the previous step, adding support for a fifth layer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1f5c79",
   "metadata": {},
   "source": [
    "[2 points] Implement an adaptive learning technique that was discussed in lecture and use it on the five layer network. Compare the performance of this model with and without the adaptive learning strategy. Do not use AdaM for the adaptive learning technique. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd9ff8",
   "metadata": {},
   "source": [
    "## Exceptional Work (1 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f45afbe",
   "metadata": {},
   "source": [
    "5000 level student: You have free reign to provide additional analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf28e1",
   "metadata": {},
   "source": [
    "One idea (required for 7000 level students):  Implement adaptive momentum (AdaM) in the five layer neural network and quantify the performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
