{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a3c5da3",
   "metadata": {},
   "source": [
    "# Lab Assignment Four: Multi-Layer Perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b34d0d",
   "metadata": {},
   "source": [
    "In this lab, we will compare the performance of multi-layer perceptrons programmed via our own various implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ab909",
   "metadata": {},
   "source": [
    "## Team Members:\n",
    "1) Mohammed Ahmed Abdelrazek Aboelela.\n",
    "\n",
    "2) Yihan Zhou\n",
    "\n",
    "3) Sofiya Chaku\n",
    "\n",
    "4) Naim Barnett"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a87aeab",
   "metadata": {},
   "source": [
    "## Dataset Description (as per Kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d42f21e",
   "metadata": {},
   "source": [
    "The data here are taken from the DP03 and DP05 tables of the 2015 American Community Survey 5-year estimates. The full datasets and much more can be found at the American Factfinder website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5815b2",
   "metadata": {},
   "source": [
    "The classification task we will be performing is to predict, for each county, what the child poverty rate will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1035d8",
   "metadata": {},
   "source": [
    "## Load, Split, and Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "baaa85df",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Importing all the required libraries throughout the lab'''\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import warnings\n",
    "import re\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a909d343",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3220 entries, 0 to 3219\n",
      "Data columns (total 37 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   CensusId         3220 non-null   int64  \n",
      " 1   State            3220 non-null   object \n",
      " 2   County           3220 non-null   object \n",
      " 3   TotalPop         3220 non-null   int64  \n",
      " 4   Men              3220 non-null   int64  \n",
      " 5   Women            3220 non-null   int64  \n",
      " 6   Hispanic         3220 non-null   float64\n",
      " 7   White            3220 non-null   float64\n",
      " 8   Black            3220 non-null   float64\n",
      " 9   Native           3220 non-null   float64\n",
      " 10  Asian            3220 non-null   float64\n",
      " 11  Pacific          3220 non-null   float64\n",
      " 12  Citizen          3220 non-null   int64  \n",
      " 13  Income           3219 non-null   float64\n",
      " 14  IncomeErr        3219 non-null   float64\n",
      " 15  IncomePerCap     3220 non-null   int64  \n",
      " 16  IncomePerCapErr  3220 non-null   int64  \n",
      " 17  Poverty          3220 non-null   float64\n",
      " 18  ChildPoverty     3219 non-null   float64\n",
      " 19  Professional     3220 non-null   float64\n",
      " 20  Service          3220 non-null   float64\n",
      " 21  Office           3220 non-null   float64\n",
      " 22  Construction     3220 non-null   float64\n",
      " 23  Production       3220 non-null   float64\n",
      " 24  Drive            3220 non-null   float64\n",
      " 25  Carpool          3220 non-null   float64\n",
      " 26  Transit          3220 non-null   float64\n",
      " 27  Walk             3220 non-null   float64\n",
      " 28  OtherTransp      3220 non-null   float64\n",
      " 29  WorkAtHome       3220 non-null   float64\n",
      " 30  MeanCommute      3220 non-null   float64\n",
      " 31  Employed         3220 non-null   int64  \n",
      " 32  PrivateWork      3220 non-null   float64\n",
      " 33  PublicWork       3220 non-null   float64\n",
      " 34  SelfEmployed     3220 non-null   float64\n",
      " 35  FamilyWork       3220 non-null   float64\n",
      " 36  Unemployment     3220 non-null   float64\n",
      "dtypes: float64(27), int64(8), object(2)\n",
      "memory usage: 930.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55221</td>\n",
       "      <td>26745</td>\n",
       "      <td>28476</td>\n",
       "      <td>2.6</td>\n",
       "      <td>75.8</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>26.5</td>\n",
       "      <td>23986</td>\n",
       "      <td>73.6</td>\n",
       "      <td>20.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>195121</td>\n",
       "      <td>95314</td>\n",
       "      <td>99807</td>\n",
       "      <td>4.5</td>\n",
       "      <td>83.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>26.4</td>\n",
       "      <td>85953</td>\n",
       "      <td>81.5</td>\n",
       "      <td>12.3</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>26932</td>\n",
       "      <td>14497</td>\n",
       "      <td>12435</td>\n",
       "      <td>4.6</td>\n",
       "      <td>46.2</td>\n",
       "      <td>46.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>24.1</td>\n",
       "      <td>8597</td>\n",
       "      <td>71.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>22604</td>\n",
       "      <td>12073</td>\n",
       "      <td>10531</td>\n",
       "      <td>2.2</td>\n",
       "      <td>74.5</td>\n",
       "      <td>21.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>28.8</td>\n",
       "      <td>8294</td>\n",
       "      <td>76.8</td>\n",
       "      <td>16.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Blount</td>\n",
       "      <td>57710</td>\n",
       "      <td>28512</td>\n",
       "      <td>29198</td>\n",
       "      <td>8.6</td>\n",
       "      <td>87.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>34.9</td>\n",
       "      <td>22189</td>\n",
       "      <td>82.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CensusId    State   County  TotalPop    Men  Women  Hispanic  White  Black  \\\n",
       "0      1001  Alabama  Autauga     55221  26745  28476       2.6   75.8   18.5   \n",
       "1      1003  Alabama  Baldwin    195121  95314  99807       4.5   83.1    9.5   \n",
       "2      1005  Alabama  Barbour     26932  14497  12435       4.6   46.2   46.7   \n",
       "3      1007  Alabama     Bibb     22604  12073  10531       2.2   74.5   21.4   \n",
       "4      1009  Alabama   Blount     57710  28512  29198       8.6   87.9    1.5   \n",
       "\n",
       "   Native  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  Employed  \\\n",
       "0     0.4  ...   0.5          1.3         1.8         26.5     23986   \n",
       "1     0.6  ...   1.0          1.4         3.9         26.4     85953   \n",
       "2     0.2  ...   1.8          1.5         1.6         24.1      8597   \n",
       "3     0.4  ...   0.6          1.5         0.7         28.8      8294   \n",
       "4     0.3  ...   0.9          0.4         2.3         34.9     22189   \n",
       "\n",
       "   PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \n",
       "0         73.6        20.9           5.5         0.0           7.6  \n",
       "1         81.5        12.3           5.8         0.4           7.5  \n",
       "2         71.8        20.8           7.3         0.1          17.6  \n",
       "3         76.8        16.1           6.7         0.4           8.3  \n",
       "4         82.0        13.5           4.2         0.4           7.7  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Loading the data into memory and saving it to a pandas dataframe\"\"\"\n",
    "data = pd.read_csv(\"Data/acs2015_county_data.csv\", low_memory=False)\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b284e3b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3218 entries, 0 to 3219\n",
      "Data columns (total 37 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   CensusId         3218 non-null   int64  \n",
      " 1   State            3218 non-null   object \n",
      " 2   County           3218 non-null   object \n",
      " 3   TotalPop         3218 non-null   int64  \n",
      " 4   Men              3218 non-null   int64  \n",
      " 5   Women            3218 non-null   int64  \n",
      " 6   Hispanic         3218 non-null   float64\n",
      " 7   White            3218 non-null   float64\n",
      " 8   Black            3218 non-null   float64\n",
      " 9   Native           3218 non-null   float64\n",
      " 10  Asian            3218 non-null   float64\n",
      " 11  Pacific          3218 non-null   float64\n",
      " 12  Citizen          3218 non-null   int64  \n",
      " 13  Income           3218 non-null   float64\n",
      " 14  IncomeErr        3218 non-null   float64\n",
      " 15  IncomePerCap     3218 non-null   int64  \n",
      " 16  IncomePerCapErr  3218 non-null   int64  \n",
      " 17  Poverty          3218 non-null   float64\n",
      " 18  ChildPoverty     3218 non-null   float64\n",
      " 19  Professional     3218 non-null   float64\n",
      " 20  Service          3218 non-null   float64\n",
      " 21  Office           3218 non-null   float64\n",
      " 22  Construction     3218 non-null   float64\n",
      " 23  Production       3218 non-null   float64\n",
      " 24  Drive            3218 non-null   float64\n",
      " 25  Carpool          3218 non-null   float64\n",
      " 26  Transit          3218 non-null   float64\n",
      " 27  Walk             3218 non-null   float64\n",
      " 28  OtherTransp      3218 non-null   float64\n",
      " 29  WorkAtHome       3218 non-null   float64\n",
      " 30  MeanCommute      3218 non-null   float64\n",
      " 31  Employed         3218 non-null   int64  \n",
      " 32  PrivateWork      3218 non-null   float64\n",
      " 33  PublicWork       3218 non-null   float64\n",
      " 34  SelfEmployed     3218 non-null   float64\n",
      " 35  FamilyWork       3218 non-null   float64\n",
      " 36  Unemployment     3218 non-null   float64\n",
      "dtypes: float64(27), int64(8), object(2)\n",
      "memory usage: 955.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55221</td>\n",
       "      <td>26745</td>\n",
       "      <td>28476</td>\n",
       "      <td>2.6</td>\n",
       "      <td>75.8</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>26.5</td>\n",
       "      <td>23986</td>\n",
       "      <td>73.6</td>\n",
       "      <td>20.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>195121</td>\n",
       "      <td>95314</td>\n",
       "      <td>99807</td>\n",
       "      <td>4.5</td>\n",
       "      <td>83.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>26.4</td>\n",
       "      <td>85953</td>\n",
       "      <td>81.5</td>\n",
       "      <td>12.3</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>26932</td>\n",
       "      <td>14497</td>\n",
       "      <td>12435</td>\n",
       "      <td>4.6</td>\n",
       "      <td>46.2</td>\n",
       "      <td>46.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>24.1</td>\n",
       "      <td>8597</td>\n",
       "      <td>71.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>22604</td>\n",
       "      <td>12073</td>\n",
       "      <td>10531</td>\n",
       "      <td>2.2</td>\n",
       "      <td>74.5</td>\n",
       "      <td>21.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>28.8</td>\n",
       "      <td>8294</td>\n",
       "      <td>76.8</td>\n",
       "      <td>16.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Blount</td>\n",
       "      <td>57710</td>\n",
       "      <td>28512</td>\n",
       "      <td>29198</td>\n",
       "      <td>8.6</td>\n",
       "      <td>87.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>34.9</td>\n",
       "      <td>22189</td>\n",
       "      <td>82.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3215</th>\n",
       "      <td>72145</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Vega Baja</td>\n",
       "      <td>56858</td>\n",
       "      <td>27379</td>\n",
       "      <td>29479</td>\n",
       "      <td>96.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>13660</td>\n",
       "      <td>78.3</td>\n",
       "      <td>17.6</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>72147</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Vieques</td>\n",
       "      <td>9130</td>\n",
       "      <td>4585</td>\n",
       "      <td>4545</td>\n",
       "      <td>96.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2860</td>\n",
       "      <td>44.5</td>\n",
       "      <td>41.6</td>\n",
       "      <td>13.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3217</th>\n",
       "      <td>72149</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Villalba</td>\n",
       "      <td>24685</td>\n",
       "      <td>12086</td>\n",
       "      <td>12599</td>\n",
       "      <td>99.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>26.9</td>\n",
       "      <td>6795</td>\n",
       "      <td>59.2</td>\n",
       "      <td>27.5</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>72151</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Yabucoa</td>\n",
       "      <td>36279</td>\n",
       "      <td>17648</td>\n",
       "      <td>18631</td>\n",
       "      <td>99.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>29.5</td>\n",
       "      <td>8083</td>\n",
       "      <td>65.1</td>\n",
       "      <td>27.6</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3219</th>\n",
       "      <td>72153</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Yauco</td>\n",
       "      <td>39474</td>\n",
       "      <td>19047</td>\n",
       "      <td>20427</td>\n",
       "      <td>99.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>24.6</td>\n",
       "      <td>8923</td>\n",
       "      <td>68.0</td>\n",
       "      <td>27.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3218 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CensusId        State     County  TotalPop    Men  Women  Hispanic  \\\n",
       "0         1001      Alabama    Autauga     55221  26745  28476       2.6   \n",
       "1         1003      Alabama    Baldwin    195121  95314  99807       4.5   \n",
       "2         1005      Alabama    Barbour     26932  14497  12435       4.6   \n",
       "3         1007      Alabama       Bibb     22604  12073  10531       2.2   \n",
       "4         1009      Alabama     Blount     57710  28512  29198       8.6   \n",
       "...        ...          ...        ...       ...    ...    ...       ...   \n",
       "3215     72145  Puerto Rico  Vega Baja     56858  27379  29479      96.4   \n",
       "3216     72147  Puerto Rico    Vieques      9130   4585   4545      96.7   \n",
       "3217     72149  Puerto Rico   Villalba     24685  12086  12599      99.7   \n",
       "3218     72151  Puerto Rico    Yabucoa     36279  17648  18631      99.8   \n",
       "3219     72153  Puerto Rico      Yauco     39474  19047  20427      99.5   \n",
       "\n",
       "      White  Black  Native  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  \\\n",
       "0      75.8   18.5     0.4  ...   0.5          1.3         1.8         26.5   \n",
       "1      83.1    9.5     0.6  ...   1.0          1.4         3.9         26.4   \n",
       "2      46.2   46.7     0.2  ...   1.8          1.5         1.6         24.1   \n",
       "3      74.5   21.4     0.4  ...   0.6          1.5         0.7         28.8   \n",
       "4      87.9    1.5     0.3  ...   0.9          0.4         2.3         34.9   \n",
       "...     ...    ...     ...  ...   ...          ...         ...          ...   \n",
       "3215    3.4    0.1     0.0  ...   1.2          1.3         0.3         32.0   \n",
       "3216    2.9    0.0     0.0  ...  10.8          0.0         1.4         14.0   \n",
       "3217    0.0    0.0     0.0  ...   3.2          0.0         3.3         26.9   \n",
       "3218    0.2    0.0     0.0  ...   2.3          2.3         1.5         29.5   \n",
       "3219    0.5    0.0     0.0  ...   1.6          0.7         3.1         24.6   \n",
       "\n",
       "      Employed  PrivateWork  PublicWork  SelfEmployed  FamilyWork  \\\n",
       "0        23986         73.6        20.9           5.5         0.0   \n",
       "1        85953         81.5        12.3           5.8         0.4   \n",
       "2         8597         71.8        20.8           7.3         0.1   \n",
       "3         8294         76.8        16.1           6.7         0.4   \n",
       "4        22189         82.0        13.5           4.2         0.4   \n",
       "...        ...          ...         ...           ...         ...   \n",
       "3215     13660         78.3        17.6           4.1         0.0   \n",
       "3216      2860         44.5        41.6          13.6         0.3   \n",
       "3217      6795         59.2        27.5          13.1         0.2   \n",
       "3218      8083         65.1        27.6           7.3         0.0   \n",
       "3219      8923         68.0        27.6           4.4         0.0   \n",
       "\n",
       "      Unemployment  \n",
       "0              7.6  \n",
       "1              7.5  \n",
       "2             17.6  \n",
       "3              8.3  \n",
       "4              7.7  \n",
       "...            ...  \n",
       "3215          15.2  \n",
       "3216          12.2  \n",
       "3217          25.9  \n",
       "3218          24.3  \n",
       "3219          27.1  \n",
       "\n",
       "[3218 rows x 37 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Removing observations that have missing data\"\"\"\n",
    "#Replace all blank strings will null to be dropped\n",
    "data.replace('', np.nan, inplace=True)\n",
    "#Remove all rows with null values\n",
    "data.dropna(inplace=True)\n",
    "#Find duplicate instances\n",
    "duplicates = data[data.duplicated()]\n",
    "#Remove all duplicates\n",
    "data = data.drop_duplicates()\n",
    "#After removal \n",
    "data.info()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9e9f05a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observations for each state before incoding was:\n",
      "Texas                   253\n",
      "Georgia                 159\n",
      "Virginia                133\n",
      "Kentucky                120\n",
      "Missouri                115\n",
      "Kansas                  105\n",
      "Illinois                102\n",
      "North Carolina          100\n",
      "Iowa                     99\n",
      "Tennessee                95\n",
      "Nebraska                 93\n",
      "Indiana                  92\n",
      "Ohio                     88\n",
      "Minnesota                87\n",
      "Michigan                 83\n",
      "Mississippi              82\n",
      "Puerto Rico              78\n",
      "Oklahoma                 77\n",
      "Arkansas                 75\n",
      "Wisconsin                72\n",
      "Pennsylvania             67\n",
      "Alabama                  67\n",
      "Florida                  67\n",
      "South Dakota             66\n",
      "Louisiana                64\n",
      "Colorado                 64\n",
      "New York                 62\n",
      "California               58\n",
      "Montana                  56\n",
      "West Virginia            55\n",
      "North Dakota             53\n",
      "South Carolina           46\n",
      "Idaho                    44\n",
      "Washington               39\n",
      "Oregon                   36\n",
      "New Mexico               33\n",
      "Alaska                   29\n",
      "Utah                     29\n",
      "Maryland                 24\n",
      "Wyoming                  23\n",
      "New Jersey               21\n",
      "Nevada                   17\n",
      "Maine                    16\n",
      "Arizona                  15\n",
      "Massachusetts            14\n",
      "Vermont                  14\n",
      "New Hampshire            10\n",
      "Connecticut               8\n",
      "Rhode Island              5\n",
      "Hawaii                    4\n",
      "Delaware                  3\n",
      "District of Columbia      1\n",
      "Name: State, dtype: int64\n",
      "The number of observations for each state after incoding becomes:\n",
      "43    253\n",
      "10    159\n",
      "46    133\n",
      "17    120\n",
      "25    115\n",
      "16    105\n",
      "13    102\n",
      "33    100\n",
      "15     99\n",
      "42     95\n",
      "27     93\n",
      "14     92\n",
      "35     88\n",
      "23     87\n",
      "22     83\n",
      "24     82\n",
      "51     78\n",
      "36     77\n",
      "3      75\n",
      "49     72\n",
      "38     67\n",
      "0      67\n",
      "9      67\n",
      "41     66\n",
      "18     64\n",
      "5      64\n",
      "32     62\n",
      "4      58\n",
      "26     56\n",
      "48     55\n",
      "34     53\n",
      "40     46\n",
      "12     44\n",
      "47     39\n",
      "37     36\n",
      "31     33\n",
      "1      29\n",
      "44     29\n",
      "20     24\n",
      "50     23\n",
      "30     21\n",
      "28     17\n",
      "19     16\n",
      "2      15\n",
      "21     14\n",
      "45     14\n",
      "29     10\n",
      "6       8\n",
      "39      5\n",
      "11      4\n",
      "7       3\n",
      "8       1\n",
      "Name: State, dtype: int64\n",
      "The number of observations for each County before incoding was:\n",
      "47           31\n",
      "Jefferson    26\n",
      "Franklin     25\n",
      "Jackson      24\n",
      "Lincoln      24\n",
      "             ..\n",
      "Gladwin       1\n",
      "Eaton         1\n",
      "Clare         1\n",
      "Cheboygan     1\n",
      "Yauco         1\n",
      "Name: County, Length: 1926, dtype: int64\n",
      "The number of observations for each County after incoding becomes:\n",
      "64      31\n",
      "36      26\n",
      "29      25\n",
      "35      24\n",
      "140     24\n",
      "        ..\n",
      "847      1\n",
      "845      1\n",
      "844      1\n",
      "842      1\n",
      "1925     1\n",
      "Name: County, Length: 1926, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3218 entries, 0 to 3219\n",
      "Data columns (total 37 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   CensusId         3218 non-null   int64  \n",
      " 1   State            3218 non-null   int64  \n",
      " 2   County           3218 non-null   int64  \n",
      " 3   TotalPop         3218 non-null   int64  \n",
      " 4   Men              3218 non-null   int64  \n",
      " 5   Women            3218 non-null   int64  \n",
      " 6   Hispanic         3218 non-null   float64\n",
      " 7   White            3218 non-null   float64\n",
      " 8   Black            3218 non-null   float64\n",
      " 9   Native           3218 non-null   float64\n",
      " 10  Asian            3218 non-null   float64\n",
      " 11  Pacific          3218 non-null   float64\n",
      " 12  Citizen          3218 non-null   int64  \n",
      " 13  Income           3218 non-null   float64\n",
      " 14  IncomeErr        3218 non-null   float64\n",
      " 15  IncomePerCap     3218 non-null   int64  \n",
      " 16  IncomePerCapErr  3218 non-null   int64  \n",
      " 17  Poverty          3218 non-null   float64\n",
      " 18  ChildPoverty     3218 non-null   float64\n",
      " 19  Professional     3218 non-null   float64\n",
      " 20  Service          3218 non-null   float64\n",
      " 21  Office           3218 non-null   float64\n",
      " 22  Construction     3218 non-null   float64\n",
      " 23  Production       3218 non-null   float64\n",
      " 24  Drive            3218 non-null   float64\n",
      " 25  Carpool          3218 non-null   float64\n",
      " 26  Transit          3218 non-null   float64\n",
      " 27  Walk             3218 non-null   float64\n",
      " 28  OtherTransp      3218 non-null   float64\n",
      " 29  WorkAtHome       3218 non-null   float64\n",
      " 30  MeanCommute      3218 non-null   float64\n",
      " 31  Employed         3218 non-null   int64  \n",
      " 32  PrivateWork      3218 non-null   float64\n",
      " 33  PublicWork       3218 non-null   float64\n",
      " 34  SelfEmployed     3218 non-null   float64\n",
      " 35  FamilyWork       3218 non-null   float64\n",
      " 36  Unemployment     3218 non-null   float64\n",
      "dtypes: float64(27), int64(10)\n",
      "memory usage: 955.3 KB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Incoding string data as integers\"\"\"\n",
    "#Incoding states' names\n",
    "print(\"The number of observations for each state before incoding was:\")\n",
    "print(data['State'].value_counts())\n",
    "states_names = data[\"State\"].unique()\n",
    "states_id = np.arange(states_names.size)\n",
    "data.replace(states_names,states_id, inplace=True)\n",
    "data.replace(data[\"State\"].unique(),states_id, inplace=True)\n",
    "print(\"The number of observations for each state after incoding becomes:\")\n",
    "print(data['State'].value_counts())\n",
    "\n",
    "#Incoding county names\n",
    "print(\"The number of observations for each County before incoding was:\")\n",
    "print(data['County'].value_counts())\n",
    "county_names = data[\"County\"].unique()\n",
    "county_id = np.arange(county_names.size)\n",
    "data.replace(county_names,county_id, inplace=True)\n",
    "print(\"The number of observations for each County after incoding becomes:\")\n",
    "print(data['County'].value_counts())\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0eddacb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43    253\n",
      "10    159\n",
      "46    133\n",
      "17    120\n",
      "25    115\n",
      "16    105\n",
      "13    102\n",
      "33    100\n",
      "15     99\n",
      "42     95\n",
      "27     93\n",
      "14     92\n",
      "35     88\n",
      "23     87\n",
      "22     83\n",
      "24     82\n",
      "51     78\n",
      "36     77\n",
      "3      75\n",
      "49     72\n",
      "38     67\n",
      "0      67\n",
      "9      67\n",
      "41     66\n",
      "18     64\n",
      "5      64\n",
      "32     62\n",
      "4      58\n",
      "26     56\n",
      "48     55\n",
      "34     53\n",
      "40     46\n",
      "12     44\n",
      "47     39\n",
      "37     36\n",
      "31     33\n",
      "1      29\n",
      "44     29\n",
      "20     24\n",
      "50     23\n",
      "30     21\n",
      "28     17\n",
      "19     16\n",
      "2      15\n",
      "21     14\n",
      "45     14\n",
      "29     10\n",
      "6       8\n",
      "39      5\n",
      "11      4\n",
      "7       3\n",
      "8       1\n",
      "Name: State, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data.replace(data[\"State\"].unique(),states_id, inplace=True)\n",
    "print(data[\"State\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "95cae079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data matrix: (3218, 36)\n",
      "The shape of the target variable: (3218,)\n",
      "The training matrix and target shapes: (2574, 36) & (2574,)\n",
      "The testing matrix and target shapes: (644, 36) & (644,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Split the dataset into 80% for training and 20% for testing.\"\"\"\n",
    "# Creating our data matrix (X) and our target variable (y) that we will work on from the dataframe we have\n",
    "X = data.copy().drop([\"ChildPoverty\"],axis=1).to_numpy()\n",
    "#y = data[['Credit_Score']].to_numpy()\n",
    "y = data.ChildPoverty.to_numpy()\n",
    "print(\"The shape of the data matrix: \" + str(X.shape))\n",
    "print(\"The shape of the target variable: \" + str(y.shape))\n",
    "# Dividing the data into training and testing data using an 80% training and 20% testing split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.copy(), y.copy(), test_size=0.2, random_state=0)\n",
    "print(\"The training matrix and target shapes: \" + str(X_train.shape)+ ' & ' + str(y_train.shape))\n",
    "print(\"The testing matrix and target shapes: \" + str(X_test.shape)+ ' & ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921cf163",
   "metadata": {},
   "source": [
    "We choose to quantize threshold for the \"ChildPoverty\" variable that equally divide the data into four classes. We do this balancing using the entire dataset altogether, and then also apply it to training and testing to have the same evaluation criteria and division, because otherwise, the thresholds for the training and the testing sets will be different and it will be meaningless to predict the target on the test set using a model that was trained for the thresholds of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1eba43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Balancing the dataset by finding quantization thresholds for the Child Poverty variable\"\"\"\n",
    "#a function that acts on the dataset, and finds the three threshold points that define the four quarters of the variable\n",
    "def get_threshold_points():\n",
    "    i = 0;\n",
    "    threshold_points = []\n",
    "    while True:\n",
    "        if int(data[data[\"ChildPoverty\"] < i][\"ChildPoverty\"].size) >= math.floor(int(data[\"ChildPoverty\"].size)/4): \n",
    "            threshold_points.append(i)\n",
    "            while True:\n",
    "                if int(data[data[\"ChildPoverty\"] < i][\"ChildPoverty\"].size) >= math.floor(int(data[\"ChildPoverty\"].size)/2): \n",
    "                    threshold_points.append(i)\n",
    "                    while True:\n",
    "                        if int(data[data[\"ChildPoverty\"] < i][\"ChildPoverty\"].size) >= math.floor(int(data[\"ChildPoverty\"].size)*3/4): \n",
    "                            threshold_points.append(i)\n",
    "                            break\n",
    "                        else: i += 0.001\n",
    "                    break\n",
    "                else: i += 0.001\n",
    "            break\n",
    "        else: i += 0.001\n",
    "\n",
    "    return threshold_points\n",
    "\n",
    "threshold_pts = get_threshold_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc2c88f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.30099999999694, 22.70000000000476, 30.00000000001368]\n",
      "The number of instances in the first quarter: 812\n",
      "The number of instances in the second quarter: 804\n",
      "The number of instances in the third quarter: 799\n",
      "The number of instances in the fourth quarter: 803\n",
      "The \"almost perfect\" number of instances tha should be in each quarter: 804\n",
      "The number of observations that set on the edge of the first quarter: 19\n"
     ]
    }
   ],
   "source": [
    "print(threshold_pts)\n",
    "first_quarter = data[data[\"ChildPoverty\"] < threshold_pts[0]]\n",
    "other_than_first = data[data[\"ChildPoverty\"] > threshold_pts[0]]\n",
    "second_quarter = other_than_first[other_than_first[\"ChildPoverty\"] < threshold_pts[1]]\n",
    "other_than_second = data[data[\"ChildPoverty\"] > threshold_pts[1]]\n",
    "third_quarter = other_than_second[other_than_second[\"ChildPoverty\"] < threshold_pts[2]]\n",
    "fourth_quarter = data[data[\"ChildPoverty\"] > threshold_pts[2]]\n",
    "print(\"The number of instances in the first quarter:\",int(first_quarter[\"ChildPoverty\"].size))\n",
    "print(\"The number of instances in the second quarter:\",int(second_quarter[\"ChildPoverty\"].size))\n",
    "print(\"The number of instances in the third quarter:\",int(third_quarter[\"ChildPoverty\"].size))\n",
    "print(\"The number of instances in the fourth quarter:\",int(fourth_quarter[\"ChildPoverty\"].size))\n",
    "print('''The \"almost perfect\" number of instances tha should be in each quarter:''',math.floor(int(data[\"ChildPoverty\"].size)/4))\n",
    "#as we can see, we can't perfectly divide them into four equal sets because on the edge, at point 16.8, we have 19 \n",
    "#observations which will all belong to the first quarter, making it exceed. Since we want to have more flexibility\n",
    "#and we want to be more inclined towards classifying people in the less poverty rate bracket, we will assign them\n",
    "#to the first quarter, but in general, it will depend on the problem in hand.5\n",
    "print(\"The number of observations that set on the edge of the first quarter:\",int(data[data[\"ChildPoverty\"] == 16.8][\"ChildPoverty\"].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b874e7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 1. ... 0. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "#Applying the quantization to our target value \"Child Poverty\" in the training set\n",
    "y_train[y_train < threshold_pts[0]] = 0\n",
    "y_train[(y_train > threshold_pts[0]) & (y_train < threshold_pts[1])] = 1\n",
    "y_train[(y_train > threshold_pts[1]) & (y_train < threshold_pts[2])] = 2\n",
    "y_train[y_train > threshold_pts[2]] = 3\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eff15191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 0. 3. 2. 2. 1. 2. 2. 1. 0. 0. 3. 1. 1. 0. 0. 1. 1. 3. 3. 2. 1. 1. 1.\n",
      " 0. 2. 3. 0. 3. 2. 3. 3. 0. 2. 2. 1. 1. 2. 2. 1. 3. 0. 2. 3. 2. 2. 0. 3.\n",
      " 3. 2. 0. 0. 3. 2. 2. 1. 3. 2. 1. 1. 1. 2. 2. 3. 1. 2. 3. 3. 0. 1. 1. 3.\n",
      " 1. 1. 3. 3. 1. 0. 1. 2. 0. 0. 1. 3. 3. 3. 2. 1. 3. 2. 1. 0. 3. 3. 0. 0.\n",
      " 1. 3. 3. 0. 2. 1. 2. 0. 1. 0. 0. 2. 2. 0. 3. 1. 0. 3. 3. 0. 3. 2. 0. 3.\n",
      " 0. 3. 0. 0. 1. 1. 1. 1. 0. 1. 2. 1. 3. 2. 1. 1. 0. 3. 3. 2. 1. 2. 0. 0.\n",
      " 1. 3. 3. 0. 2. 0. 2. 0. 3. 0. 1. 1. 1. 1. 3. 2. 3. 0. 1. 1. 0. 3. 2. 1.\n",
      " 0. 3. 3. 3. 0. 1. 2. 1. 2. 2. 3. 3. 3. 0. 2. 3. 2. 0. 1. 2. 1. 3. 3. 3.\n",
      " 1. 0. 1. 0. 1. 0. 3. 1. 1. 3. 2. 2. 0. 2. 3. 1. 2. 1. 0. 1. 3. 2. 3. 1.\n",
      " 1. 3. 3. 2. 3. 1. 1. 0. 2. 1. 3. 0. 1. 3. 1. 3. 2. 2. 0. 3. 3. 2. 2. 3.\n",
      " 2. 3. 1. 2. 3. 0. 3. 1. 2. 0. 1. 3. 1. 1. 1. 0. 1. 1. 0. 3. 0. 3. 3. 0.\n",
      " 1. 1. 3. 3. 0. 2. 2. 0. 0. 0. 3. 3. 1. 2. 2. 2. 3. 0. 3. 2. 2. 1. 3. 2.\n",
      " 0. 1. 2. 3. 0. 0. 2. 3. 3. 3. 1. 1. 1. 1. 3. 1. 3. 3. 0. 2. 1. 0. 3. 2.\n",
      " 1. 1. 3. 3. 0. 3. 1. 3. 2. 1. 3. 0. 1. 3. 2. 2. 1. 1. 0. 0. 0. 3. 0. 1.\n",
      " 2. 3. 1. 0. 3. 0. 0. 3. 1. 2. 1. 0. 0. 0. 0. 1. 0. 0. 2. 2. 0. 1. 0. 2.\n",
      " 3. 2. 3. 2. 3. 3. 3. 1. 3. 0. 0. 2. 2. 1. 0. 3. 3. 2. 3. 2. 1. 1. 3. 0.\n",
      " 0. 0. 0. 2. 1. 2. 1. 2. 0. 1. 3. 0. 2. 2. 2. 0. 2. 0. 0. 0. 1. 0. 2. 3.\n",
      " 1. 2. 2. 0. 3. 3. 3. 2. 3. 2. 2. 1. 3. 2. 2. 1. 3. 3. 1. 0. 0. 3. 0. 2.\n",
      " 2. 0. 2. 1. 0. 2. 0. 2. 2. 0. 1. 3. 3. 3. 2. 2. 0. 2. 2. 0. 1. 3. 1. 2.\n",
      " 0. 0. 0. 1. 1. 1. 3. 1. 3. 3. 2. 0. 0. 3. 0. 1. 3. 2. 3. 1. 3. 3. 0. 1.\n",
      " 2. 1. 1. 3. 0. 0. 2. 1. 2. 1. 1. 2. 1. 3. 1. 0. 1. 2. 0. 2. 2. 3. 1. 2.\n",
      " 2. 3. 2. 2. 2. 0. 2. 3. 1. 1. 1. 2. 1. 1. 1. 2. 0. 3. 1. 2. 2. 3. 2. 0.\n",
      " 0. 0. 0. 1. 3. 2. 3. 0. 2. 2. 3. 0. 0. 2. 2. 1. 2. 0. 0. 1. 3. 3. 3. 1.\n",
      " 1. 3. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 3. 2. 3. 2. 0. 1. 1. 1. 1. 0. 3. 2.\n",
      " 0. 3. 0. 1. 3. 2. 1. 3. 3. 3. 0. 0. 3. 2. 2. 2. 3. 3. 0. 0. 3. 1. 1. 3.\n",
      " 2. 0. 1. 1. 3. 3. 0. 0. 3. 0. 1. 1. 1. 2. 1. 1. 2. 3. 1. 2. 2. 1. 1. 2.\n",
      " 3. 3. 2. 0. 3. 2. 3. 0. 1. 0. 2. 1. 1. 1. 3. 3. 2. 1. 2. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Applying the quantization to our target value \"Child Poverty\" in the testing set\n",
    "y_test[y_test < threshold_pts[0]] = 0\n",
    "y_test[(y_test > threshold_pts[0]) & (y_test < threshold_pts[1])] = 1\n",
    "y_test[(y_test > threshold_pts[1]) & (y_test < threshold_pts[2])] = 2\n",
    "y_test[y_test > threshold_pts[2]] = 3\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83132541",
   "metadata": {},
   "source": [
    "## Pre-processing (2.5 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8b2a0c",
   "metadata": {},
   "source": [
    "We use the two layer perceptron classes from class that have: (1) vectorized computation, (2) mini-batching, (3) cross entropy loss, and (4) proper Glorot initialization. We also use the ones that do Adaptive learning rate and momentum  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0f91df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Using the code from the lecture\"\"\"\n",
    "\n",
    "\"\"\"Defining the base class of the neural network\"\"\"\n",
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_) # reshape to be W\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden)\n",
    "        b2 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, b1, b2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2, b1, b2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "        \n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C\n",
    "\n",
    "        return gradW1, gradW2, gradb1, gradb2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2, self.b1, self.b2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d6973caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving the class with feedforward, fit and predict\"\"\"\n",
    "# now let's add in the following functions:\n",
    "#    feedforward\n",
    "#    fit and predict\n",
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1, W2, b1, b2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # need to vectorize this computation!\n",
    "        # See additional code and derivation below!\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2, self.b1, self.b2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X_data,self.W1,self.W2, self.b1, self.b2)\n",
    "            \n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            gradW1, gradW2, gradb1, gradb2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, Y_enc=Y_enc,\n",
    "                                              W1=self.W1, W2=self.W2)\n",
    "\n",
    "            self.W1 -= self.eta * gradW1\n",
    "            self.W2 -= self.eta * gradW2\n",
    "            self.b1 -= self.eta * gradb1\n",
    "            self.b2 -= self.eta * gradb2\n",
    "            \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "03f9484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving the class with Vectorized inputs and outputs and calculations\"\"\"\n",
    "class TwoLayerPerceptronVectorized(TwoLayerPerceptron):\n",
    "    # just need a different gradient calculation\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "        \n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C * 2\n",
    "        gradW2 += W2 * self.l2_C * 2 \n",
    "\n",
    "        return gradW1, gradW2, gradb1, gradb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b91e8357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving the class with mini-batching added\"\"\"\n",
    "class TLPMiniBatch(TwoLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.1, \n",
    "                 decrease_iter = 10, shuffle=True, \n",
    "                 minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.decrease_iter = decrease_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "\n",
    "        # start momentum at zero for previous updates\n",
    "        rho_W1_prev = np.zeros(self.W1.shape) # for momentum\n",
    "        rho_W2_prev = np.zeros(self.W2.shape) # for momentum\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            self.val_cost_ = []\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # decrease at certain epochs\n",
    "            eta = self.eta * self.decrease_const**(np.floor(i/self.decrease_iter))\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                       self.b1,\n",
    "                                                       self.b2\n",
    "                                                      )\n",
    "                \n",
    "                cost = self._cost(A3,Y_enc[:, idx],self.W1,self.W2)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradb1, gradb2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, \n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1,W2=self.W2)\n",
    "\n",
    "                # momentum calculations\n",
    "                rho_W1, rho_W2 = eta * gradW1, eta * gradW2\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev)) # update with momentum\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev)) # update with momentum\n",
    "                self.b1 -= eta * gradb1\n",
    "                self.b2 -= eta * gradb2\n",
    "                rho_W1_prev, rho_W2_prev = rho_W1, rho_W2\n",
    "                \n",
    "\n",
    "            self.cost_.append(np.mean(mini_cost))\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                yhat = self.predict(X_test)\n",
    "                self.val_score_.append(accuracy_score(y_test,yhat))\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "da352907",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving the class with cross entropy added\"\"\"\n",
    "class TLPMiniBatchCrossEntropy(TLPMiniBatch):\n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = (A3-Y_enc) # <- this is only line that changed\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "        \n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C\n",
    "\n",
    "        return gradW1, gradW2, gradb1, gradb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "337da750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving a class with Glorot Initialization\"\"\"\n",
    "class TLPBetterInitial(TLPMiniBatchCrossEntropy):             \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_))\n",
    "\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden)) \n",
    "        \n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        b2 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "88ebbdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 200/200"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.36024844720496896\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Training on our data\"\"\"\n",
    "vals = {'n_hidden':50, \n",
    "         'C':1e-2, 'epochs':200, 'eta':0.01, \n",
    "         'alpha':0.1, 'decrease_const':0.1,\n",
    "         'decrease_iter':20,\n",
    "         'minibatches':len(X_train)/256,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn = TLPBetterInitial(**vals)\n",
    "\n",
    "nn.fit(X_train, y_train, print_progress=1)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d82e3826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Plotting the cost function versus the number of epochs\"\"\" ##inspired by the lecture too\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5c0e7",
   "metadata": {},
   "source": [
    "Normalizing the continuous numeric feature data using standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5c719f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 200/200"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7484472049689441\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAbqUlEQVR4nO3dfbRddX3n8ff3nHuTm0ASHnJ5ygNBRS1UEAzgWG2htgq0U9Q6ClK1VpqFS3zqagsuZ7Rr6VpTh2mHukSzqDLU1orOiMhYBB3qw0zxgeDwFBANoCQGSHhMeMjDvfc7f5ydcHJzn0Ky7/ld9vu11l3n7H322ft79j3J5/72/u3fjsxEkqTStHpdgCRJYzGgJElFMqAkSUUyoCRJRTKgJElF6ut1AXtq4cKFuWzZsl6XIUnaR26++eaHM3Nw9PwZF1DLli1j1apVvS5DkrSPRMQvx5rvIT5JUpEMKElSkQwoSVKRDChJUpEMKElSkQwoSVKRDChJUpEMKElSkRoXUF9ZtZYPX3Vbr8uQJE2icQG1+ldP8M07Hux1GZKkSTQuoNqtFsPD3kVYkkpXW0BFxOURsSEi7phkuZMiYjgi3lxXLd362sHQiAElSaWrswV1BXD6RAtERBv4JHB9jXXsot0Khg0oSSpebQGVmd8HHp1ksfcBXwU21FXHaH2tYGhkZLo2J0l6jnp2DioiFgFvBFZOYdkVEbEqIlZt3Lhxr7bbbgUjCSO2oiSpaL3sJHEJcGFmDk+2YGZelpnLM3P54OBu97TaI32tAGA4DShJKlkvb1i4HLgyIgAWAmdGxFBmXl3nRtutTiYPjyT97Tq3JEnaGz0LqMw8asfziLgC+Ebd4QTPtqDsySdJZastoCLiS8CpwMKIWAd8DOgHyMxJzzvVpb3jEJ/XQklS0WoLqMw8Zw+W/eO66hitr72jBWVPPkkqWQNHkvAQnyTNBI0LKM9BSdLM0LiA2tmLz3NQklS0xgXUsy0oz0FJUsmaF1BVJwnH45OksjUvoDwHJUkzQuMCqnskCUlSuRoXULagJGlmaFxA7RxJwk4SklS0xgXUzhaU3cwlqWiNC6hnW1AGlCSVrHEB9exYfAaUJJWscQFlLz5JmhkaF1D24pOkmaFxAWUvPkmaGRoXULagJGlmaFxA2YtPkmaGxgVUX9VJwuugJKlsjQuotqOZS9KM0LiA8hyUJM0MjQsoe/FJ0szQuICyBSVJM0PjAspefJI0MzQuoHb24jOgJKlojQsoW1CSNDM0LqC8H5QkzQyNC6hWK4iwF58kla62gIqIyyNiQ0TcMc7r50bEbdXPjRFxfF21jNbXCs9BSVLh6mxBXQGcPsHr9wG/lZnHAR8HLquxll20W+E5KEkqXF9dK87M70fEsglev7Fr8ofA4rpqGa2v1bIFJUmFK+Uc1LuBb473YkSsiIhVEbFq48aNe70xW1CSVL6eB1REnEYnoC4cb5nMvCwzl2fm8sHBwb3eZucclJ0kJKlktR3im4qIOA74HHBGZj4yXdu1BSVJ5etZCyoilgJXAW/PzJ9N57b7WsF2r4OSpKLV1oKKiC8BpwILI2Id8DGgHyAzVwIfBQ4GPhMRAEOZubyuerq127agJKl0dfbiO2eS188Dzqtr+xOxF58kla/nnSR6oXMOyk4SklSyRgZUXysci0+SCtfMgPIclCQVr5EB1fYclCQVr5EB1ed1UJJUvEYGVNuRJCSpeI0MKFtQklS+RgZU2/tBSVLxGhlQtqAkqXyNDKh2q+V1UJJUuEYGlC0oSSpfIwOq3bYXnySVrpEBZQtKksrXyICyF58kla+RAWULSpLK18iAciw+SSpfIwPKFpQkla+RAdVuBUPD9uKTpJI1MqBsQUlS+RoZUJ3roAwoSSpZIwPKFpQkla+RAbWjF1+mISVJpWpkQPW1AgAbUZJUrkYGVLsKKMfjk6RyNTKgdrSgPA8lSeVqZEA924IyoCSpVI0MqJ0tKG9aKEnFqi2gIuLyiNgQEXeM83pExKciYk1E3BYRJ9ZVy2jtdudj24KSpHLV2YK6Ajh9gtfPAI6uflYAn62xll14DkqSyldbQGXm94FHJ1jkLOAL2fFD4ICIOLyuerrZi0+SytfLc1CLgLVd0+uqebuJiBURsSoiVm3cuHGvN2wLSpLK18uAijHmjZkYmXlZZi7PzOWDg4N7veEdLajtdpKQpGL1MqDWAUu6phcD66djw32tzse2BSVJ5eplQF0DvKPqzfdK4InMfGA6Nuw5KEkqX19dK46ILwGnAgsjYh3wMaAfIDNXAtcCZwJrgKeBd9VVy2ieg5Kk8tUWUJl5ziSvJ/DeurY/kXbbkSQkqXSNHEmi33NQklS8RgbUznNQ9uKTpGI1MqD62p6DkqTSNTKg7MUnSeVrZEDZi0+SytfIgPJ+UJJUvkYGlCNJSFL5GhlQtqAkqXyNDKhnz0HZSUKSStXIgPI6KEkqXyMDyuugJKl8jQwoz0FJUvkaGVD24pOk8jUyoGxBSVL5GhlQ9uKTpPI1MqBsQUlS+aYUUBHxj1OZN1PsbEHZzVySijXVFtSx3RMR0QZese/LmR62oCSpfBMGVER8OCI2A8dFxKbqZzOwAfj6tFRYg4ig3Qp78UlSwSYMqMz8z5k5D7g4M+dXP/My8+DM/PA01ViLditsQUlSwaZ6iO8bEbEfQET8UUT8bUQcWWNdtetrhb34JKlgUw2ozwJPR8TxwF8CvwS+UFtV08AWlCSVbaoBNZSZCZwF/F1m/h0wr76y6tfnOShJKlrfFJfbHBEfBt4OvKbqxddfX1n1a7datqAkqWBTbUG9FdgK/ElmPggsAi6urapp0NcKr4OSpIJNKaCqUPoisCAifh/Ykpkz/hzU9mE7SUhSqaY6ksRbgB8D/wF4C/CjiHhznYXVbXZ/i60GlCQVa6qH+D4CnJSZ78zMdwAnA/9psjdFxOkRcXdErImIi8Z4fUFE/K+IuDUiVkfEu/as/OduoK/N1u0GlCSVaqoB1crMDV3Tj0z23qojxaXAGcAxwDkRccyoxd4L3JmZxwOnAn8TEbOmWNNemd3fYuvQ8HRsSpL0HEy1F991EXE98KVq+q3AtZO852RgTWbeCxARV9Lppn5n1zIJzIuIAPYHHgWGpljTXpnd17IFJUkFmzCgIuJFwKGZ+RcR8Sbg1UAAP6DTaWIii4C1XdPrgFNGLfNp4BpgPZ3rqt6ambulRkSsAFYALF26dJLNTs1Af5tHn9q2T9YlSdr3JjvEdwmwGSAzr8rMP8vMD9FpPV0yyXtjjHmj+3W/HrgFOAJ4OfDpiJi/25syL8vM5Zm5fHBwcJLNTo0tKEkq22QBtSwzbxs9MzNXAcsmee86YEnX9GI6LaVu7wKuyo41wH3ASydZ7z4x0N/2HJQkFWyygBqY4LU5k7z3JuDoiDiq6vhwNp3Ded3uB14LEBGHAi8B7p1kvfvE7L4WW2xBSVKxJguomyLiT0fPjIh3AzdP9MbMHAIuAK4H7gK+kpmrI+L8iDi/WuzjwKsi4nbgBuDCzHx4Tz/EczG7zxaUJJVssl58HwS+FhHn8mwgLQdmAW+cbOWZeS2jevtl5squ5+uB1+1BvfvMQL8tKEkq2YQBlZkP0WnhnAb8ejX7XzLzX2uvrGY7WlCZSaeXuySpJFO6DiozvwN8p+ZaptVAf4uRhKGRpL9tQElSaaY6ksTzzuy+NgBbtnseSpJK1NiAGujvfPStQ56HkqQSNTagbEFJUtmaG1C2oCSpaM0NqKoF5XBHklSm5gZU1YLa4sW6klSkxgbUgC0oSSpaYwPKFpQkla25AdVXdZKwBSVJRWpsQA30V4f4bEFJUpEaG1C2oCSpbI0NKFtQklS2xgbUjhaUt9yQpDI1OKBsQUlSyRobUP3toBUOdSRJpWpsQEUEs/vaDhYrSYVqbEBB55YbtqAkqUyNDihbUJJUrmYHlC0oSSpWowNqoK/thbqSVKhGB9Ts/paDxUpSoRodULagJKlcjQ4oW1CSVK5mB5QtKEkqVrMDqr/lUEeSVKhaAyoiTo+IuyNiTURcNM4yp0bELRGxOiK+V2c9o83uazlYrCQVqq+uFUdEG7gU+F1gHXBTRFyTmXd2LXMA8Bng9My8PyIOqauesQz0t70OSpIKVWcL6mRgTWbem5nbgCuBs0Yt8zbgqsy8HyAzN9RYz25m97XY6kgSklSkOgNqEbC2a3pdNa/bi4EDI+K7EXFzRLyjxnp2YwtKkspV2yE+IMaYl2Ns/xXAa4E5wA8i4oeZ+bNdVhSxAlgBsHTp0n1W4Oy+FtuGRxgZSVqtscqVJPVKnS2odcCSrunFwPoxlrkuM5/KzIeB7wPHj15RZl6Wmcszc/ng4OA+K/DZmxbaipKk0tQZUDcBR0fEURExCzgbuGbUMl8HXhMRfRExFzgFuKvGmnYx0N/5+HY1l6Ty1HaILzOHIuIC4HqgDVyemasj4vzq9ZWZeVdEXAfcBowAn8vMO+qqabQdLSi7mktSeeo8B0VmXgtcO2reylHTFwMX11nHeGxBSVK5mj2ShOegJKlYDQ+ozsf3rrqSVJ5GB9RAvy0oSSpVowNqdr8tKEkqVaMDamDHOSh78UlScRodUHNmdT7+U9uGelyJJGm0RgfUIfMHANiwaWuPK5EkjdbogJo3u4/9ZrVZ/8QzvS5FkjRKowMqIjhswQAPPrGl16VIkkZpdEABHHHAHB4woCSpOI0PqMPm24KSpBI1PqAOXzDAhs1bGBq2q7kklaTxAXXYgjmMJGzYbE8+SSpJ4wPq8AM6Xc09DyVJZTGgFuwIKLuaS1JJDKj5cwDsKCFJhWl8QM2f08ec/raH+CSpMI0PqIjg8AUDHuKTpMI0PqCg01HCFpQklcWAAg6bP4f7H3maC//nbfz3f7uv1+VIkjCggE5Pvkee2saXV63lyh+v7XU5kiSgr9cFlOCNJy5i69Aw6x/fwnfv3kBmEhG9LkuSGs0WFPDCwf35yO8dw4lHHshT24Z57OntvS5JkhrPgOqy5MDONVFrH326x5VIkgyoLksOmgvA2scMKEnqNQOqy+KqBbXuMa+JkqReM6C6zBvo54C5/R7ik6QCGFCjLDlwLmttQUlSz9UaUBFxekTcHRFrIuKiCZY7KSKGI+LNddYzFUsOmsM6W1CS1HO1BVREtIFLgTOAY4BzIuKYcZb7JHB9XbXsiSUHzmXdY88wMpK9LkWSGq3OFtTJwJrMvDcztwFXAmeNsdz7gK8CG2qsZcoWHziHbcMjbHzSO+xKUi/VGVCLgO5xg9ZV83aKiEXAG4GVE60oIlZExKqIWLVx48Z9Xmi3xTu6mnuYT5J6qs6AGmusoNHHzS4BLszM4YlWlJmXZebyzFw+ODi4r+ob05IDOwF1vwElST1V51h864AlXdOLgfWjllkOXFmNe7cQODMihjLz6hrrmtCSg+awcP9Z/PU3f8qRB+/H3Q9uZr/Zbc56+aLJ3yxJ2mfqDKibgKMj4ijgV8DZwNu6F8jMo3Y8j4grgG/0MpwAZve1+eJ5r+Ttn/8Rf/jZGwGY1dfitJcewvyB/l6WJkmNUtshvswcAi6g0zvvLuArmbk6Is6PiPPr2u6+8JLD5vHV97yK9/32i/jrN72MbUMjXHfHg70uS5IaJTJnVnfq5cuX56pVq6Zte5nJqf/1uyw5cC7/dN4p07ZdSWqKiLg5M5ePnu9IEpOICM46/ghuvOdhPvd/7uWsS//NHn6SNA0MqCk464RFjCR84l/u4ta1j/Pdu4u4ZEuSnte8o+4UvHBwfy447UUs3H8Wl9zwc+58YFOvS5Kk5z0Daor+/PUvAeBbdz7E6vUGlCTVzUN8e+jYI+bz0wc3s314pNelSNLzmgG1h449YgHbhka4Z+OTvS5Fkp7XDKg9dOwR8wFY/SsP80lSnQyoPfSCwf0Z6G95HkqSamZA7aF2K3jpYfNZvf6JXpciSc9rBtRzcOwR81m9fhNPPL2916VI0vOWAfUcnH3SUrZsH+YjV9/OTBsqSpJmCq+Deg5etngBH/rdF3Px9Xdz5MFzOf3Yw1m9/glW/fIxHn96GwvmzOK0lw7y+mMPo7/t3wCS9Fw4WOxzNDySrPjCKm746bPDHi3cfzaHzJvN+iee4fGnt/NHr1zKJ97wsh5WKUnlG2+wWFtQz1G7FXz+j09i7aNPc/MvH+PFh87j1w6fR0QwPJL8x6tv58ofr+W8V7+AZQv363W5kjTjePxpLy05aC5vOGERxxwxn+rOwLRbwYd+58X0tYNL/vfPelyhJM1MtqBqcsj8Ad71G0ex8nv38NS2YV53zKG89tcO5aD9ZvW6NEmaEQyoGl1w2ovYNjTCN29/gG/f+RCtgJOPOojXH3sYZ5+0lDmz2r0uUZKKZSeJaZCZrF6/ietXP8i3Vj/E3Q9t5oSlB/D5d55ki0pS443XScKA6oHr7niAD1x5C0ccMIcvnncKRxwwp9clSVLPeMv3gpz+64fzz396Cg9v3srZl/2Q29c9wcbNW9k6NNzr0iSpGLageuiWtY/z9s//iM1bhnbOm93XYv6cfuYN9DF/oJ92q9MzcMf0QH+Lgf42A/1tZve1qp82s/u7nve1mDXO/M70s+/t80JiST3mdVAFevmSA7j2/a/hJ/c/xqZntrNpy9Czj1u2s+mZ7YxUf0A88uQ27nv4KbZuH2Hr0DBbqseRvfz74sC5/Rw6f4B2K4iAYMcjEEF0HqrH7unOzF2mdzzvWg+7vW/X6amZ2oJTXd9UNzv19U2+4JTXtQ+3WS24LxeTdnPuKUfy7154cC3rNqB6bMlBc1ly0Nzn9N7MZGgk2To0wtbtw53HoU5wdYJshC3bh9k2NMK24V3n7wi5hzZtYcPmrYyMJFmts/PIzml2Tmfnsfs5kCNQzdnlfd3rYZfpznJT+4xTXG4P9tm+NJXV5RSr69VnnfIeSUwy7eaJZ7bVtm4DagaLCPrbQX+7xf6z/VVKen7xBIQkqUgGlCSpSAaUJKlItQZURJweEXdHxJqIuGiM18+NiNuqnxsj4vg665EkzRy1BVREtIFLgTOAY4BzIuKYUYvdB/xWZh4HfBy4rK56JEkzS50tqJOBNZl5b2ZuA64EzupeIDNvzMzHqskfAotrrEeSNIPUGVCLgLVd0+uqeeN5N/DNsV6IiBURsSoiVm3cuHEflihJKlWdATXWJX1jXhMYEafRCagLx3o9My/LzOWZuXxwcHAflihJKlWdV3euA5Z0TS8G1o9eKCKOAz4HnJGZj9RYjyRpBqltsNiI6AN+BrwW+BVwE/C2zFzdtcxS4F+Bd2TmjVNc70bgl3tZ3kLg4b1cx3SZKbXOlDrBWuswU+oEa63D3tZ5ZGbudnisthZUZg5FxAXA9UAbuDwzV0fE+dXrK4GPAgcDn4nOKJlDY41oO2q9e32MLyJWTbadUsyUWmdKnWCtdZgpdYK11qGuOmsdwC0zrwWuHTVvZdfz84Dz6qxBkjQzOZKEJKlITQ2omXRB8EypdabUCdZah5lSJ1hrHWqpc8bdUVeS1AxNbUFJkgpnQEmSitS4gJpshPVeiYglEfGdiLgrIlZHxAeq+X8VEb+KiFuqnzN7XStARPwiIm6valpVzTsoIr4dET+vHg8soM6XdO27WyJiU0R8sIT9GhGXR8SGiLija964+zAiPlx9b++OiNcXUOvFEfHT6m4EX4uIA6r5yyLima59u3LcFU9freP+vnu1X8ep88tdNf4iIm6p5vd6n473/1O939fMbMwPneux7gFeAMwCbgWO6XVdVW2HAydWz+fRucj5GOCvgD/vdX1j1PsLYOGoef8FuKh6fhHwyV7XOcbv/0HgyBL2K/CbwInAHZPtw+q7cCswGziq+h63e1zr64C+6vknu2pd1r1cIft1zN93L/frWHWOev1vgI8Wsk/H+/+p1u9r01pQk46w3iuZ+UBm/qR6vhm4i4kH1y3RWcA/VM//AXhD70oZ02uBezJzb0ci2Scy8/vAo6Nmj7cPzwKuzMytmXkfsIbO93lajFVrZn4rM4eqyWLuRjDOfh1Pz/brRHVGZ+SCtwBfmo5aJjPB/0+1fl+bFlB7OsJ6T0TEMuAE4EfVrAuqwyiXl3DYrJLAtyLi5ohYUc07NDMfgM4XGjikZ9WN7Wx2/Qdf4n4dbx+W/t39E3a9G8FREfH/IuJ7EfGaXhU1yli/71L362uAhzLz513zitino/5/qvX72rSAmvII670SEfsDXwU+mJmbgM8CLwReDjxAp9lfgt/IzBPp3JDyvRHxm70uaCIRMQv4A+B/VLNK3a/jKfa7GxEfAYaAL1azHgCWZuYJwJ8B/xwR83tVX2W833ep+/Ucdv1jqoh9Osb/T+MuOsa8Pd6vTQuoKY2w3isR0U/nl//FzLwKIDMfyszhzBwB/p5pPKwzkcxcXz1uAL5Gp66HIuJwgOpxQ+8q3M0ZwE8y8yEod78y/j4s8rsbEe8Efh84N6uTD9VhnUeq5zfTOf/w4t5VOeHvu7j9Gp2Btt8EfHnHvBL26Vj/P1Hz97VpAXUTcHREHFX9RX02cE2PawJ2HnP+PHBXZv5t1/zDuxZ7I3DH6PdOt4jYLyLm7XhO52T5HXT25Turxd4JfL03FY5pl79IS9yvlfH24TXA2RExOyKOAo4GftyD+naKiNPp3MPtDzLz6a75gxHRrp6/gE6t9/amyp01jff7Lm6/Ar8D/DQz1+2Y0et9Ot7/T9T9fe1Vr5Be/QBn0umBcg/wkV7X01XXq+k0gW8Dbql+zgT+Ebi9mn8NcHgBtb6ATg+dW4HVO/YjnZHpbwB+Xj0e1Otaq7rmAo8AC7rm9Xy/0gnMB4DtdP7ifPdE+xD4SPW9vZvO/dN6XesaOucZdnxfV1bL/mH1vbgV+Anw7wuoddzfd6/261h1VvOvAM4ftWyv9+l4/z/V+n11qCNJUpGadohPkjRDGFCSpCIZUJKkIhlQkqQiGVCSpCIZUFINImI4dh1FfZ+NnF+NbF3KdVtSbfp6XYD0PPVMZr6810VIM5ktKGkaVff4+WRE/Lj6eVE1/8iIuKEazPSGiFhazT80OvdaurX6eVW1qnZE/H11b55vRcScavn3R8Sd1Xqu7NHHlPYJA0qqx5xRh/je2vXapsw8Gfg0cEk179PAFzLzODqDrn6qmv8p4HuZeTydewetruYfDVyamccCj9MZaQA69+Q5oVrP+fV8NGl6OJKEVIOIeDIz9x9j/i+A387Me6vBNx/MzIMj4mE6w+9sr+Y/kJkLI2IjsDgzt3atYxnw7cw8upq+EOjPzE9ExHXAk8DVwNWZ+WTNH1WqjS0oafrlOM/HW2YsW7ueD/Ps+eTfAy4FXgHcXI2MLc1IBpQ0/d7a9fiD6vmNdEbXBzgX+L/V8xuA9wBERHuiewBFRAtYkpnfAf4SOADYrRUnzRT+dSXVY05E3NI1fV1m7uhqPjsifkTnD8RzqnnvBy6PiL8ANgLvquZ/ALgsIt5Np6X0HjojYI+lDfxTRCygc8O4/5aZj++jzyNNO89BSdOoOge1PDMf7nUtUuk8xCdJKpItKElSkWxBSZKKZEBJkopkQEmSimRASZKKZEBJkor0/wH8+3XkkqF+9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Normalizing the continuous numeric feature data using standardization\"\"\"\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "nn = TLPBetterInitial(**vals)\n",
    "\n",
    "nn.fit(X_train, y_train, print_progress=50)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test,yhat))\n",
    "\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e2fae9d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 200/200"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.31832298136645965\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"One Hot Encoding the data we have\"\"\"\n",
    "#getting back the original categories for the dataframe\n",
    "data[\"State\"].replace(states_id, states_names, inplace=True)\n",
    "data[\"County\"].replace(county_id, county_names, inplace=True)\n",
    "data.head()\n",
    "\n",
    "#doing the one-hot encoding and reconstructing the data matrix\n",
    "X = pd.get_dummies(data.copy())\n",
    "X = X.drop([\"ChildPoverty\"],axis=1).to_numpy()\n",
    "\n",
    "#Constructing the new split using the new one-hot encoded data matrix\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.copy(), y.copy(), test_size=0.2, random_state=0)\n",
    "\n",
    "#Applying the quantization to our target value \"Child Poverty\" in the training set\n",
    "y_train[y_train < threshold_pts[0]] = 0\n",
    "y_train[(y_train > threshold_pts[0]) & (y_train < threshold_pts[1])] = 1\n",
    "y_train[(y_train > threshold_pts[1]) & (y_train < threshold_pts[2])] = 2\n",
    "y_train[y_train > threshold_pts[2]] = 3\n",
    "\n",
    "#Applying the quantization to our target value \"Child Poverty\" in the testing set\n",
    "y_test[y_test < threshold_pts[0]] = 0\n",
    "y_test[(y_test > threshold_pts[0]) & (y_test < threshold_pts[1])] = 1\n",
    "y_test[(y_test > threshold_pts[1]) & (y_test < threshold_pts[2])] = 2\n",
    "y_test[y_test > threshold_pts[2]] = 3\n",
    "\n",
    "nn = TLPBetterInitial(**vals)\n",
    "\n",
    "nn.fit(X_train, y_train, print_progress=50)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test,yhat))\n",
    "\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecbd7fa",
   "metadata": {},
   "source": [
    "Looking at the accuracy scores and the loss function graphs versus the number of epochs, it is very clear that we get the best performance when only applying the normalization to the data matrix. This should be expected because we center the feature columns at mean 0 with standard\n",
    "deviation 1 so that the feature columns have the same parameters as a standard\n",
    "normal distribution (zero mean and unit variance), which makes it easier to learn\n",
    "the weights. However, the worst performance happened when we applied the one-hot encoding besdies the normalization. this is probably because of the very high number of features we are deadling with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe4f0c",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c766f5c",
   "metadata": {},
   "source": [
    "We know implement the three layer perceptron. We do so while keeping our minimum requirements of doing: (1) vectorized computation, (2) mini-batching, (3) cross entropy loss, and (4) proper Glorot initialization. This will require many changes in each of the four classes functions, we will indicate changes whenever happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7af5baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden_1=30, n_hidden_2=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        \"\"\"Added a container for the number of units in the extra hidden layer\"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden_1 = n_hidden_1\n",
    "        self.n_hidden_2 = n_hidden_2\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_)*self.n_hidden_1\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden_1, self.n_features_) # reshape to be W\n",
    "        b1 = np.zeros((self.n_hidden_1, 1))\n",
    "        \n",
    "        \"\"\"Initializing the weights for the new added third layer\"\"\"\n",
    "\n",
    "        W2_num_elems = (self.n_hidden_1)*self.n_hidden_2\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_hidden_2, self.n_hidden_1) # reshape to be W\n",
    "        b2 = np.zeros((self.n_hidden_2, 1))\n",
    "        \n",
    "        W3_num_elems = (self.n_hidden_2)*self.n_output_\n",
    "        W3 = np.random.uniform(-1.0, 1.0, size=W3_num_elems)\n",
    "        W3 = W3.reshape(self.n_output_, self.n_hidden_2)\n",
    "        b3 = np.zeros((self.n_output_, 1))\n",
    "\n",
    "\n",
    "        \n",
    "        return W1, W2, W3, b1, b2, b3\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2, W3):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        \"\"\"Including the mean of the new added third layer\"\"\"\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2) + np.mean(W3[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A4,Y_enc,W1,W2,W3):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A4)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3) #notice the new function input referring to the new layer\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2, W3, b1, b2, b3):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> 1st hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> 2nd hidden layer.\n",
    "        W3: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a4 : activations into layer (or output layer)\n",
    "        z1-z3 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        Z3 = W3 @ A3 + b3\n",
    "        A4 = self._sigmoid(Z3)\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A4\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, A4, Z1, Z2, Z3, Y_enc, W1, W2, W3):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V3 = -2*(Y_enc-A4)*A4*(1-A4)\n",
    "        V2 = A3*(1-A3)*(W3.T @ V3) # The new added third layer\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        gradW3 = V3 @ A3.T\n",
    "        gradW2 = V2 @ A2.T     #the gradient of the new layer\n",
    "        gradW1 = V1 @ A1.T\n",
    "\n",
    "        gradb3 = np.sum(V3, axis=1).reshape((-1,1))\n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))     #the bias of the new layer\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C       #regularizing the weight for the new layer\n",
    "        gradW3 += W3 * self.l2_C \n",
    "\n",
    "        return gradW1, gradW2, gradW3, gradb1, gradb2, gradb3\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, _, _, A4 = self._feedforward(X, self.W1, self.W2, self.W3, self.b1, self.b2, self.b3)\n",
    "        y_pred = np.argmax(A4, axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d559e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving the class with mini-batching added\"\"\"\n",
    "class ThreeLPMiniBatch(ThreeLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.1, \n",
    "                 decrease_iter = 10, shuffle=True, \n",
    "                 minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.decrease_iter = decrease_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3, self.b1, self.b2, self.b3 = self._initialize_weights()\n",
    "        \n",
    "        \n",
    "        #initializing containers of the average weight for each epoch\n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.grad_w3_ = np.zeros(self.epochs)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            self.val_cost_ = []\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "            eta = self.eta\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "\n",
    "            #Containers for the average weight per batch to be averaged to the average per epoch\n",
    "            self.grad_w1_batch = []\n",
    "            self.grad_w2_batch = []\n",
    "            self.grad_w3_batch = []\n",
    "\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3, Z3, A4 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                       self.W3,\n",
    "                                                       self.b1,\n",
    "                                                       self.b2,\n",
    "                                                       self.b3\n",
    "                                                      )\n",
    "                \n",
    "                cost = self._cost(A4,Y_enc[:, idx],self.W1,self.W2,self.W3)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradW3, gradb1, gradb2, gradb3 = self._get_gradient(A1=A1, A2=A2, A3=A3, A4=A4, Z1=Z1, Z2=Z2, Z3=Z3, \n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1,W2=self.W2,W3=self.W3)\n",
    "                \n",
    "                #adding the mean of the gradients to their respective containers to be averaged later\n",
    "                self.grad_w1_batch.append(gradW1.mean())\n",
    "                self.grad_w2_batch.append(gradW2.mean())\n",
    "                self.grad_w3_batch.append(gradW3.mean())\n",
    "\n",
    "                #Updating the weights\n",
    "                self.W1 -= self.eta * gradW1\n",
    "                self.W2 -= self.eta * gradW2\n",
    "                self.W3 -= self.eta * gradW3\n",
    "                self.b1 -= eta * gradb1\n",
    "                self.b2 -= eta * gradb2\n",
    "                self.b3 -= eta * gradb3\n",
    "                \n",
    "            #Converting to numpy arrrays to be able to calculate the means of the batch gradients for every epoch\n",
    "            self.grad_w1_batch = np.array(self.grad_w1_batch)\n",
    "            self.grad_w2_batch = np.array(self.grad_w2_batch)\n",
    "            self.grad_w3_batch = np.array(self.grad_w3_batch)\n",
    "            self.grad_w1_[i] =  self.grad_w1_batch.mean()\n",
    "            self.grad_w2_[i] =  self.grad_w2_batch.mean()\n",
    "            self.grad_w3_[i] =  self.grad_w3_batch.mean()\n",
    "            \n",
    "            \n",
    "\n",
    "            self.cost_.append(np.mean(mini_cost))\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                yhat = self.predict(X_test)\n",
    "                self.val_score_.append(accuracy_score(y_test,yhat))\n",
    "        \n",
    "        #Plotting the average gradient magnitude for each epoch\n",
    "        ax = plt.subplot(1,1,1)\n",
    "        plt.plot(abs(self.grad_w1_[10:]), label='w1')\n",
    "        plt.plot(abs(self.grad_w2_[10:]), label='w2')\n",
    "        plt.plot(abs(self.grad_w3_[10:]), label='w3')\n",
    "        plt.legend()\n",
    "        plt.ylabel('Average gradient magnitude')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.show()\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f6c32e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving the class with cross entropy added\"\"\"\n",
    "class ThreeLPMiniBatchCrossEntropy(ThreeLPMiniBatch):\n",
    "    def _cost(self,A4,Y_enc,W1,W2,W3):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A4)+(1-Y_enc)*np.log(1-A4))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    # def _get_gradient(self, A1, A2, A3, A4, Z1, Z2, Z3, Y_enc, W1, W2, W3):\n",
    "    #     \"\"\" Compute gradient step using backpropagation.\n",
    "    #     \"\"\"\n",
    "    #     # vectorized backpropagation\n",
    "    #     V3 = (A4-Y_enc) # <- this is only line that changed\n",
    "    #     V2 = A3*(1-A3)*(W3.T @ V3)\n",
    "    #     V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "    #     gradW3 = V3 @ A3.T\n",
    "    #     gradW2 = V2 @ A2.T\n",
    "    #     gradW1 = V1 @ A1.T\n",
    "\n",
    "    #     gradb3 = np.sum(V3, axis=1).reshape((-1,1))\n",
    "    #     gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "    #     gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        \n",
    "    #     # regularize weights that are not bias terms\n",
    "    #     gradW1 += W1 * self.l2_C\n",
    "    #     gradW2 += W2 * self.l2_C\n",
    "    #     gradW3 += W3 * self.l2_C\n",
    "\n",
    "    #     return gradW1, gradW2, gradW3, gradb1, gradb2, gradb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "14b6df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving a class with Glorot Initialization\"\"\"\n",
    "class ThreeLPBetterInitial(ThreeLPMiniBatchCrossEntropy):             \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden_1 + self.n_features_))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden_1, self.n_features_))\n",
    "\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_hidden_2 + self.n_hidden_1))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_hidden_2, self.n_hidden_1))\n",
    "\n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden_2))\n",
    "        W3 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden_2))\n",
    "        \n",
    "        b1 = np.zeros((self.n_hidden_1, 1))\n",
    "        b2 = np.zeros((self.n_hidden_2, 1))\n",
    "        b3 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, W3, b1, b2, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f31790bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 200/200"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2375776397515528\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Training on our data\"\"\"\n",
    "#notice the new \"n_hidden_2\" in the initialization to control the number of units in the hidden layer\n",
    "vals_new = {'n_hidden_1':50, 'n_hidden_2':50,\n",
    "         'C':1e-2, 'epochs':200, 'eta':0.01, \n",
    "         'alpha':0.1, 'decrease_const':0.1,\n",
    "         'decrease_iter':20,\n",
    "         'minibatches':int(len(X_train)/2),\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn = ThreeLPBetterInitial(**vals_new)\n",
    "\n",
    "nn.fit(X_train, y_train, print_progress=50)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test,yhat))\n",
    "\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a8315",
   "metadata": {},
   "source": [
    "[1 points] Repeat the previous step, adding support for a fourth layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a1f2c84f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:12\u001b[0;36m\u001b[0m\n\u001b[0;31m    @staticmethod\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class FourLayerPerceptronBase(object):\n",
    "     def __init__(self, n_hidden_1=30, n_hidden_2=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        \"\"\"Added a container for the number of units in the extra hidden layer\"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden_1 = n_hidden_1\n",
    "        self.n_hidden_2 = n_hidden_2\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_)*self.n_hidden_1\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden_1, self.n_features_) # reshape to be W\n",
    "        b1 = np.zeros((self.n_hidden_1, 1))\n",
    "        \n",
    "        \"\"\"Initializing the weights for the new added third layer\"\"\"\n",
    "\n",
    "        W2_num_elems = (self.n_hidden_1)*self.n_hidden_2\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_hidden_2, self.n_hidden_1) # reshape to be W\n",
    "        b2 = np.zeros((self.n_hidden_2, 1))\n",
    "        \n",
    "        W3_num_elems = (self.n_hidden_2)*self.n_output_\n",
    "        W3 = np.random.uniform(-1.0, 1.0, size=W3_num_elems)\n",
    "        W3 = W3.reshape(self.n_output_, self.n_hidden_2)\n",
    "        b3 = np.zeros((self.n_output_, 1))\n",
    "\n",
    "\n",
    "        \n",
    "        return W1, W2, W3, b1, b2, b3\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2, W3):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        \"\"\"Including the mean of the new added third layer\"\"\"\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2) + np.mean(W3[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A4,Y_enc,W1,W2,W3):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A4)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3) #notice the new function input referring to the new layer\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2, W3, b1, b2, b3):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> 1st hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> 2nd hidden layer.\n",
    "        W3: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a4 : activations into layer (or output layer)\n",
    "        z1-z3 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        Z3 = W3 @ A3 + b3\n",
    "        A4 = self._sigmoid(Z3)\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A4\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, A4, Z1, Z2, Z3, Y_enc, W1, W2, W3):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V3 = -2*(Y_enc-A4)*A4*(1-A4)\n",
    "        V2 = A3*(1-A3)*(W3.T @ V3) # The new added third layer\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        gradW3 = V3 @ A3.T\n",
    "        gradW2 = V2 @ A2.T     #the gradient of the new layer\n",
    "        gradW1 = V1 @ A1.T\n",
    "\n",
    "        gradb3 = np.sum(V3, axis=1).reshape((-1,1))\n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))     #the bias of the new layer\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C       #regularizing the weight for the new layer\n",
    "        gradW3 += W3 * self.l2_C \n",
    "\n",
    "        return gradW1, gradW2, gradW3, gradb1, gradb2, gradb3\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, _, _, A4 = self._feedforward(X, self.W1, self.W2, self.W3, self.b1, self.b2, self.b3)\n",
    "        y_pred = np.argmax(A4, axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92cb0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving the class with mini-batching added\"\"\"\n",
    "class FourLPMiniBatch(FourLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.1, \n",
    "                 decrease_iter = 10, shuffle=True, \n",
    "                 minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.decrease_iter = decrease_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3, self.b1, self.b2, self.b3 = self._initialize_weights()\n",
    "\n",
    "        # # start momentum at zero for previous updates\n",
    "        # rho_W1_prev = np.zeros(self.W1.shape) # for momentum\n",
    "        # rho_W2_prev = np.zeros(self.W2.shape) # for momentum\n",
    "        # rho_W3_prev = np.zeros(self.W3.shape) # for momentum\n",
    "\n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.grad_w3_ = np.zeros(self.epochs)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            self.val_cost_ = []\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # decrease at certain epochs\n",
    "            # eta = self.eta * self.decrease_const**(np.floor(i/self.decrease_iter))\n",
    "            eta = self.eta\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "\n",
    "            self.grad_w1_batch = []\n",
    "            self.grad_w2_batch = []\n",
    "            self.grad_w3_batch = []\n",
    "\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3, Z3, A4 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                       self.W3,\n",
    "                                                       self.b1,\n",
    "                                                       self.b2,\n",
    "                                                       self.b3\n",
    "                                                      )\n",
    "                \n",
    "                cost = self._cost(A4,Y_enc[:, idx],self.W1,self.W2,self.W3)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradW3, gradb1, gradb2, gradb3 = self._get_gradient(A1=A1, A2=A2, A3=A3, A4=A4, Z1=Z1, Z2=Z2, Z3=Z3, \n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1,W2=self.W2,W3=self.W3)\n",
    "\n",
    "                self.grad_w1_batch.append(gradW1.mean())\n",
    "                self.grad_w2_batch.append(gradW2.mean())\n",
    "                self.grad_w3_batch.append(gradW3.mean())\n",
    "\n",
    "                # # momentum calculations\n",
    "                # rho_W1, rho_W2, rho_W3 = eta * gradW1, eta * gradW2, eta * gradW3\n",
    "                # self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev)) # update with momentum\n",
    "                # self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev)) # update with momentum\n",
    "                # self.W3 -= (rho_W3 + (self.alpha * rho_W3_prev)) # update with momentum\n",
    "                self.W1 -= self.eta * gradW1\n",
    "                self.W2 -= self.eta * gradW2\n",
    "                self.W3 -= self.eta * gradW3\n",
    "                self.b1 -= eta * gradb1\n",
    "                self.b2 -= eta * gradb2\n",
    "                self.b3 -= eta * gradb3\n",
    "                # rho_W1_prev, rho_W2_prev, rho_W3_prev = rho_W1, rho_W2, rho_W3\n",
    "                \n",
    "            self.grad_w1_batch = np.array(self.grad_w1_batch)\n",
    "            self.grad_w2_batch = np.array(self.grad_w2_batch)\n",
    "            self.grad_w3_batch = np.array(self.grad_w3_batch)\n",
    "            self.grad_w1_[i] =  self.grad_w1_batch.mean()\n",
    "            self.grad_w2_[i] =  self.grad_w2_batch.mean()\n",
    "            self.grad_w3_[i] =  self.grad_w3_batch.mean()\n",
    "            \n",
    "            \n",
    "\n",
    "            self.cost_.append(np.mean(mini_cost))\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                yhat = self.predict(X_test)\n",
    "                self.val_score_.append(accuracy_score(y_test,yhat))\n",
    "\n",
    "        ax = plt.subplot(1,1,1)\n",
    "        plt.plot(abs(self.grad_w1_[10:]), label='w1')\n",
    "        plt.plot(abs(self.grad_w2_[10:]), label='w2')\n",
    "        plt.plot(abs(self.grad_w3_[10:]), label='w3')\n",
    "        plt.legend()\n",
    "        plt.ylabel('Average gradient magnitude')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.show()\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919da72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving the class with cross entropy added\"\"\"\n",
    "class FourLPMiniBatchCrossEntropy(FourLPMiniBatch):\n",
    "    def _cost(self,A4,Y_enc,W1,W2,W3):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A4)+(1-Y_enc)*np.log(1-A4))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    # def _get_gradient(self, A1, A2, A3, A4, Z1, Z2, Z3, Y_enc, W1, W2, W3):\n",
    "    #     \"\"\" Compute gradient step using backpropagation.\n",
    "    #     \"\"\"\n",
    "    #     # vectorized backpropagation\n",
    "    #     V3 = (A4-Y_enc) # <- this is only line that changed\n",
    "    #     V2 = A3*(1-A3)*(W3.T @ V3)\n",
    "    #     V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "    #     gradW3 = V3 @ A3.T\n",
    "    #     gradW2 = V2 @ A2.T\n",
    "    #     gradW1 = V1 @ A1.T\n",
    "\n",
    "    #     gradb3 = np.sum(V3, axis=1).reshape((-1,1))\n",
    "    #     gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "    #     gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        \n",
    "    #     # regularize weights that are not bias terms\n",
    "    #     gradW1 += W1 * self.l2_C\n",
    "    #     gradW2 += W2 * self.l2_C\n",
    "    #     gradW3 += W3 * self.l2_C\n",
    "\n",
    "    #     return gradW1, gradW2, gradW3, gradb1, gradb2, gradb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b2e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving a class with Glorot Initialization\"\"\"\n",
    "class FourLPBetterInitial(FourLPMiniBatchCrossEntropy):             \n",
    "     def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden_1 + self.n_features_))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden_1, self.n_features_))\n",
    "\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_hidden_2 + self.n_hidden_1))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_hidden_2, self.n_hidden_1))\n",
    "\n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden_2))\n",
    "        W3 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden_2))\n",
    "        \n",
    "        b1 = np.zeros((self.n_hidden_1, 1))\n",
    "        b2 = np.zeros((self.n_hidden_2, 1))\n",
    "        b3 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, W3, b1, b2, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0408195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training on our data\"\"\"\n",
    "vals_new = {'n_hidden_1':50, 'n_hidden_2':50,\n",
    "         'C':1e-2, 'epochs':200, 'eta':0.01, \n",
    "         'alpha':0.1, 'decrease_const':0.1,\n",
    "         'decrease_iter':20,\n",
    "         'minibatches':int(len(X_train)/2),\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn = ThreeLPBetterInitial(**vals_new)\n",
    "\n",
    "nn.fit(X_train, y_train, print_progress=50)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test,yhat))\n",
    "\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3eb84f",
   "metadata": {},
   "source": [
    "[1 points] Repeat the previous step, adding support for a fifth layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1129fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiveLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden_1=30, n_hidden_2=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden_1 = n_hidden_1\n",
    "        self.n_hidden_2 = n_hidden_2\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_)*self.n_hidden_1\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden_1, self.n_features_) # reshape to be W\n",
    "        b1 = np.zeros((self.n_hidden_1, 1))\n",
    "\n",
    "        W2_num_elems = (self.n_hidden_1)*self.n_hidden_2\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_hidden_2, self.n_hidden_1) # reshape to be W\n",
    "        b2 = np.zeros((self.n_hidden_2, 1))\n",
    "        \n",
    "        W3_num_elems = (self.n_hidden_2)*self.n_output_\n",
    "        W3 = np.random.uniform(-1.0, 1.0, size=W3_num_elems)\n",
    "        W3 = W3.reshape(self.n_output_, self.n_hidden_2)\n",
    "        b3 = np.zeros((self.n_output_, 1))\n",
    "\n",
    "\n",
    "        \n",
    "        return W1, W2, W3, b1, b2, b3\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2, W3):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2) + np.mean(W3[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A4,Y_enc,W1,W2,W3):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A4)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2, W3, b1, b2, b3):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> 1st hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> 2nd hidden layer.\n",
    "        W3: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a4 : activations into layer (or output layer)\n",
    "        z1-z3 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        Z3 = W3 @ A3 + b3\n",
    "        A4 = self._sigmoid(Z3)\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A4\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, A4, Z1, Z2, Z3, Y_enc, W1, W2, W3):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V3 = -2*(Y_enc-A4)*A4*(1-A4)\n",
    "        V2 = A3*(1-A3)*(W3.T @ V3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        gradW3 = V3 @ A3.T\n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "\n",
    "        gradb3 = np.sum(V3, axis=1).reshape((-1,1))\n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C\n",
    "        gradW3 += W3 * self.l2_C\n",
    "\n",
    "        return gradW1, gradW2, gradW3, gradb1, gradb2, gradb3\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, _, _, A4 = self._feedforward(X, self.W1, self.W2, self.W3, self.b1, self.b2, self.b3)\n",
    "        y_pred = np.argmax(A4, axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e63aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving the class with mini-batching added\"\"\"\n",
    "class FiveLPMiniBatch(FiveLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.1, \n",
    "                 decrease_iter = 10, shuffle=True, \n",
    "                 minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.decrease_iter = decrease_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3, self.b1, self.b2, self.b3 = self._initialize_weights()\n",
    "\n",
    "        # start momentum at zero for previous updates\n",
    "        rho_W1_prev = np.zeros(self.W1.shape) # for momentum\n",
    "        rho_W2_prev = np.zeros(self.W2.shape) # for momentum\n",
    "        rho_W3_prev = np.zeros(self.W3.shape) # for momentum\n",
    "\n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.grad_w3_ = np.zeros(self.epochs)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            self.val_cost_ = []\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # decrease at certain epochs\n",
    "            eta = self.eta * self.decrease_const**(np.floor(i/self.decrease_iter))\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "\n",
    "            self.grad_w1_batch = np.zeros(self.minibatches)\n",
    "            self.grad_w2_batch = np.zeros(self.minibatches)\n",
    "            self.grad_w3_batch = np.zeros(self.minibatches)\n",
    "\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3, Z3, A4 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                       self.W3,\n",
    "                                                       self.b1,\n",
    "                                                       self.b2,\n",
    "                                                       self.b3\n",
    "                                                      )\n",
    "                \n",
    "                cost = self._cost(A4,Y_enc[:, idx],self.W1,self.W2,self.W3)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradW3, gradb1, gradb2, gradb3 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, Z3=Z3, \n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1,W2=self.W2,W3=self.W3)\n",
    "\n",
    "                self.grad_w1_batch[idx] = gradW1.mean()\n",
    "                self.grad_w2_batch[idx] = gradW2.mean()\n",
    "                self.grad_w3_batch[idx] = gradW3.mean()\n",
    "\n",
    "                # momentum calculations\n",
    "                rho_W1, rho_W2, rho_W3 = eta * gradW1, eta * gradW2, eta * gradW3\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev)) # update with momentum\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev)) # update with momentum\n",
    "                self.W3 -= (rho_W3 + (self.alpha * rho_W3_prev)) # update with momentum\n",
    "                self.b1 -= eta * gradb1\n",
    "                self.b2 -= eta * gradb2\n",
    "                self.b3 -= eta * gradb3\n",
    "                rho_W1_prev, rho_W2_prev, rho_W3_prev = rho_W1, rho_W2, rho_W3\n",
    "                \n",
    "            self.grad_w1_[i] =  self.grad_w1_batch.mean()\n",
    "            self.grad_w2_[i] =  self.grad_w2_batch.mean()\n",
    "            self.grad_w3_[i] =  self.grad_w3_batch.mean()\n",
    "\n",
    "            self.cost_.append(np.mean(mini_cost))\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                yhat = self.predict(X_test)\n",
    "                self.val_score_.append(accuracy_score(y_test,yhat))\n",
    "\n",
    "        ax = plt.subplot(1,1,1)\n",
    "        plt.plot(abs(self.grad_w1_[10:]), label='w1')\n",
    "        plt.plot(abs(self.grad_w2_[10:]), label='w2')\n",
    "        plt.plot(abs(self.grad_w3_[10:]), label='w3')\n",
    "        plt.legend()\n",
    "        plt.ylabel('Average gradient magnitude')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.show()\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab162352",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving the class with cross entropy added\"\"\"\n",
    "class FiveLPMiniBatchCrossEntropy(FiveLPMiniBatch):\n",
    "    def _cost(self,A4,Y_enc,W1,W2,W3):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A4)+(1-Y_enc)*np.log(1-A4))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    # def _get_gradient(self, A1, A2, A3, A4, Z1, Z2, Z3, Y_enc, W1, W2, W3):\n",
    "    #     \"\"\" Compute gradient step using backpropagation.\n",
    "    #     \"\"\"\n",
    "    #     # vectorized backpropagation\n",
    "    #     V3 = (A4-Y_enc) # <- this is only line that changed\n",
    "    #     V2 = A3*(1-A3)*(W3.T @ V3)\n",
    "    #     V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "    #     gradW3 = V3 @ A3.T\n",
    "    #     gradW2 = V2 @ A2.T\n",
    "    #     gradW1 = V1 @ A1.T\n",
    "\n",
    "    #     gradb3 = np.sum(V3, axis=1).reshape((-1,1))\n",
    "    #     gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "    #     gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        \n",
    "    #     # regularize weights that are not bias terms\n",
    "    #     gradW1 += W1 * self.l2_C\n",
    "    #     gradW2 += W2 * self.l2_C\n",
    "    #     gradW3 += W3 * self.l2_C\n",
    "\n",
    "    #     return gradW1, gradW2, gradW3, gradb1, gradb2, gradb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2adf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deriving a class with Glorot Initialization\"\"\"\n",
    "class FiveLPBetterInitial(FiveLPMiniBatchCrossEntropy):             \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden_1 + self.n_features_))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden_1, self.n_features_))\n",
    "\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_hidden_2 + self.n_hidden_1))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_hidden_2, self.n_hidden_1))\n",
    "\n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden_2))\n",
    "        W3 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden_2))\n",
    "        \n",
    "        b1 = np.zeros((self.n_hidden_1, 1))\n",
    "        b2 = np.zeros((self.n_hidden_2, 1))\n",
    "        b3 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, W3, b1, b2, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3346df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training on our data\"\"\"\n",
    "vals = {'n_hidden':50, \n",
    "         'C':1e-2, 'epochs':200, 'eta':0.01, \n",
    "         'alpha':0.1, 'decrease_const':0.1,\n",
    "         'decrease_iter':20,\n",
    "         'minibatches':len(X_train)/256,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn = TLPBetterInitial(**vals)\n",
    "\n",
    "nn.fit(X_train, y_train, print_progress=50)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test,yhat))\n",
    "\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1f5c79",
   "metadata": {},
   "source": [
    "[2 points] Implement an adaptive learning technique that was discussed in lecture and use it on the five layer network. Compare the performance of this model with and without the adaptive learning strategy. Do not use AdaM for the adaptive learning technique. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd9ff8",
   "metadata": {},
   "source": [
    "## Exceptional Work (1 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f45afbe",
   "metadata": {},
   "source": [
    "5000 level student: You have free reign to provide additional analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf28e1",
   "metadata": {},
   "source": [
    "One idea (required for 7000 level students):  Implement adaptive momentum (AdaM) in the five layer neural network and quantify the performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5e760e3d3e77aa6826ef4f79166ae45bbfcee688d0233fb93e5618e0e742b754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
